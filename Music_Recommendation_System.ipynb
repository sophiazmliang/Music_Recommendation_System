{"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sophiazmliang/music-recommendation-system?scriptVersionId=190616125\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Music Recommendation System**","metadata":{"id":"DyAjEw4OHmDb"}},{"cell_type":"markdown","source":"## **Problem Definition**\n\n### **The Context:**\n\n  Online streaming platforms like Spotify offer millions of songs in their music library. Users often feel overwhelmed by the sheer volume of choices. Developing a recommendation system that suggests relevant songs based on users' historical interactions can significantly enhance user engagement. This increased engagement encourages users to spend more time on the platform, leading to higher user satisfaction and increased subscription renewals or higher advertising revenue from free-tier users, thus improving the platform's overall revenue.\n\n  Moreover, such recommendation systems can aid in discovering emerging music trends, guiding strategic decisions on content acquisition, partnerships, and marketing. Continuous improvement and innovation in recommendation algorithms can differentiate a platform from its competitors, establishing leadership in the tech and music streaming industries.\n  \n\n### **The objective:**\n\n The primary goal of this analysis is to build a recommendation system that proposes the top 5 songs for a user based on the likelihood of listening to those songs.\n\n This system aims to enhance the user experience by providing personalized content that aligns with individual preferences, allowing users to discover new music they might not have found on their own, thereby enriching their overall experience.\n\n  By achieving this goal, the system seeks to increase user engagement, satisfaction, and retention, ultimately driving higher subscription renewals and advertising revenue. Additionally, the system can help management identify emerging music trends and market patterns, enabling strategic decision-making.\n\n\n### **The key questions:**\n\n- What user/song data is available and how can it be used?\n- What algorithms and techniques (e.g., collaborative filtering, content-based filtering, hybrid methods) can be used?\n- How can user interactions with songs be quantified and used to train the model?\n- How to handle data sparsity?\n- How to evaluate the recommendation system? What metrics (e.g., precision, recall, F1 score, RMSE) should be used?\n- How to address potential biases in the recommendation algorithm to ensure fairness?\n- How can the system help identify emerging music trends for strategic decision-making?\n\n### **The problem formulation**:\n\n- Data Collection and Preprocessing: Gathering data on user interactions, song features, and metadata. Cleaning and preprocessing this data to make it suitable for analysis.\n- Modeling: Using machine learning algorithms (e.g., collaborative filtering, content-based filtering, hybrid models) to predict user preferences.\n- Evaluation: Measuring the accuracy and effectiveness of the recommendations using metrics like precision, recall, and RMSE.\n- Optimization: Tuning the models to improve performance and scalability.\n","metadata":{"id":"FMCaC7Q_tq1m"}},{"cell_type":"markdown","source":"## **Data Dictionary**\n\nThe core data is the Taste Profile Subset released by the Echo Nest as part of the Million Song Dataset. There are two files in this dataset. The first file contains the details about the song id, titles, release, artist name, and the year of release. The second file contains the user id, song id, and the play count of users.\n\n**song_data**\n\n- song_id - A unique id given to every song\n- title - Title of the song\n- Release - Name of the released album\n- Artist_name - Name of the artist\n- year - Year of release\n\n**count_data**\n\n- user _id - A unique id given to the user\n- song_id - A unique id given to the song\n- play_count - Number of times the song was played\n\n## **Data Source**\nhttp://millionsongdataset.com/","metadata":{"id":"BVUiyhYTHS1t"}},{"cell_type":"code","source":"# Mounting the drive\nfrom google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"6SRzOPXI2Efn","executionInfo":{"status":"ok","timestamp":1718343667664,"user_tz":240,"elapsed":2533,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"4acc5eae-230e-4a1a-e7e2-0220538069f4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Importing Libraries and the Dataset**","metadata":{"id":"NRJtXkTrHxMQ"}},{"cell_type":"code","source":"# Used to ignore the warning given as output of the code\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Basic libraries of python for numeric and dataframe computations\nimport numpy as np\nimport pandas as pd\n\n# Set the display width to a larger value\npd.set_option('display.width', 200)  # Adjust the value as needed\n\n# Import Matplotlib the Basic library for data visualization\nimport matplotlib.pyplot as plt\n\n# Import seaborn - Slightly advanced library for data visualization\nimport seaborn as sns\n\n# Import the required library to compute the cosine similarity between two vectors\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Import defaultdict from collections A dictionary output that does not raise a key error\nfrom collections import defaultdict\n\n# Impoort mean_squared_error : a performance metrics in sklearn\nfrom sklearn.metrics import mean_squared_error","metadata":{"id":"R4YvKrpzId3K","executionInfo":{"status":"ok","timestamp":1718343667665,"user_tz":240,"elapsed":2,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Load the dataset**","metadata":{"id":"bUGKX140wf-S"}},{"cell_type":"code","source":"# Importing the datasets\ncount_df = pd.read_csv('/content/drive/MyDrive/Colab/Capstone_Project/count_data.csv')\nsong_df = pd.read_csv('/content/drive/MyDrive/Colab/Capstone_Project/song_data.csv')","metadata":{"id":"si6ulhIYImck","executionInfo":{"status":"ok","timestamp":1718343671545,"user_tz":240,"elapsed":3882,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Understanding the data by viewing a few observations**","metadata":{"id":"12TKB2M7XyC6"}},{"cell_type":"code","source":"# Display first 10 records of count_df data\ncount_df.head(10)","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1718343671546,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"GCLzBuYiXlPM","outputId":"b46a14ec-f601-486a-d5be-f85b2c8efc91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 records of song_df data\nsong_df.head(10)","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718343671547,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"tV1ed0ApXpu3","outputId":"f99dec65-8133-46cb-8f3a-882b0fc87b0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Let us check the data types and and missing values of each column**","metadata":{"id":"bvKb5FHcXzcN"}},{"cell_type":"code","source":"# Display info of count_df\ncount_df.info()","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718343671547,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"yyoHc_cnX19J","outputId":"1b748825-bec2-44c6-e238-c224eb31fa2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Non-Null Count is not showed, maybe due to large dataset. Check directly.\ncount_df.isnull().sum()","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1718343671547,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"E9gP57ldoasW","outputId":"dd32035b-d864-4b37-be46-988558d87e78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display info of song_df\nsong_df.info()","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1718343671877,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"rz3zDx_LX42y","outputId":"fab583c3-1671-4b98-9d2c-a3231ed3b8a6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the unique values of song_id in song_df\nsong_df['song_id'].nunique()","metadata":{"executionInfo":{"elapsed":571,"status":"ok","timestamp":1718343672447,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"6dBDVAKlZsc0","outputId":"d055694d-b107-4e83-99e5-eb558af8fefa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the unique values of user_id in count_df\ncount_df['user_id'].nunique()","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718343672447,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"mRXaUSjeZ7t_","outputId":"737b3c63-3572-44a9-cc94-3956618416c6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the unique values of song_id in count_df\ncount_df['song_id'].nunique()","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718343672447,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"f9a5mQOuaGGz","outputId":"f1cc4c21-916f-48dd-ce6c-a57a353d81f8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by user_id and song_id and count the number of unique pairs in count_df\nunique_pairs_alt = count_df.groupby(['user_id', 'song_id']).size().reset_index(name='counts')\n\n# Count the number of unique pairs\nlen(unique_pairs_alt)","metadata":{"executionInfo":{"elapsed":2245,"status":"ok","timestamp":1718343674690,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"39bidYptxuIR","outputId":"284f1119-0ad7-47df-b4a0-0e2a355ae512"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Observations and Insights:**\n\n\n*   Both datasets are very large. There are 70,000+ users and almost 100,000 songs.\n*   Only 10,000 songs are played, we can remove the irrelevant songs to trim down dataset size to 10%.\n*   The 'Unnamed: 0' column is no use, can drop it later.\n*   'play_count' and 'year' are numeric columns, the others are all string columns.\n*   There are some missing values in song_df. Further check it after the big trim.\n\n*   There are some duplicate (song_id) records in song_df. We can further clean it after the big trim.\n*   Some song_id records have 0 in 'year' column. Need to minimize it during clean up.\n*   No duplicate (user_id + song_id) record in count_df.\n\n\n\n\n","metadata":{"id":"Ze2TlWxpYadn"}},{"cell_type":"markdown","source":"### **Trim down the dataset to a more manageable size and do some cleanup**","metadata":{"id":"RmrGFDQ85-Ub"}},{"cell_type":"markdown","source":"**Remove the irrelevant songs.**","metadata":{"id":"4pOg2Q2Wp0-X"}},{"cell_type":"code","source":"# Remove those song_id records which do not appear in count_df\nsong_df = song_df[song_df['song_id'].isin(count_df['song_id'])]\n\n# Check song_id unique values afterwards\nsong_df['song_id'].nunique()","metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1718343674690,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"3EbaSTUQMWfp","outputId":"e7339a39-11bf-4dbd-b4c3-57dbbfdb89c1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check song_df info() again.\nsong_df.info()","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718343674690,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"7v0w86E_MwO5","outputId":"580f420e-d19c-4c6d-aa2c-44b9c63a401d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After trim, the no. of unique song_id in song_df matches with count_df, also there are no missing values.**\n\n**Now let's handle the duplicate records and 0-year issues.**","metadata":{"id":"1TZ5T47oryWQ"}},{"cell_type":"code","source":"# Divide into two datasets, one with non-zero year and one with zero year\nsong_with_year_df = song_df[song_df.year != 0]\nsong_without_year_df = song_df[song_df.year == 0]\n\n# Drop duplicate song_id in both sets\nsong_with_year_df = song_with_year_df.drop_duplicates(subset='song_id')\nsong_without_year_df = song_without_year_df.drop_duplicates(subset='song_id')\n\n# Remove those zero-year records if their song_id exist in non-zero-year dataset\nsong_without_year_df = song_without_year_df[~song_without_year_df['song_id'].isin(song_with_year_df['song_id'])]\n\n# Combine the two datasets again and display info()\nsong_df = pd.concat([song_with_year_df,song_without_year_df],axis=0)\nsong_df.info()","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718343674690,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"_BwyoASyQCl_","outputId":"c1d42163-7e95-4e53-f7a4-acd0c7c444c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The clean up for song_df complete. Now merge it with count_df and drop the column 'Unnamed: 0'.**","metadata":{"id":"tmgzkjZgxloP"}},{"cell_type":"code","source":"# Left merge count_df and song_df on \"song_id\". Name the obtained dataframe as \"df\"\ndf = pd.merge(count_df, song_df, on = 'song_id', how = 'left')\n\n# Drop the column 'Unnamed: 0'\ndf = df.drop(['Unnamed: 0'], axis = 1)","metadata":{"id":"oTeurvID2T9U","executionInfo":{"status":"ok","timestamp":1718343675899,"user_tz":240,"elapsed":6,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 5 records of df data\ndf.head()","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718343675899,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"BJcYiCeBf1K1","outputId":"5a4f95aa-2e42-4d9e-ca4a-dab3413270f7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It:** As the user_id and song_id are encrypted. Can they be encoded to numeric features?","metadata":{"id":"yWeY9ZT43XFX"}},{"cell_type":"markdown","source":"**Encode user_id and song_id in df and song_df.**","metadata":{"id":"5NNxf1vW6mp6"}},{"cell_type":"code","source":"# Import LableEncoder\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"id":"AyqalASHhc9B","executionInfo":{"status":"ok","timestamp":1718343676385,"user_tz":240,"elapsed":6,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply label encoding for \"user_id\" and \"song_id\"\n\n# Initialize LabelEncoders\nuser_id_encoder = LabelEncoder()\nsong_id_encoder = LabelEncoder()\n\n# Fit and transform the 'user_id' column\ndf['user_id'] = user_id_encoder.fit_transform(df['user_id'])\n\n# Fit and transform the 'song_id' column\ndf['song_id'] = song_id_encoder.fit_transform(df['song_id'])\n\n# Display first 5 records of df data\ndf.head()","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718343676385,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"oxeoOVxh2T9U","outputId":"6fef66bd-c03e-47df-b02a-2005d651d97a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform the 'song_id' column in song_df too with the same encoder\nsong_df['song_id'] = song_id_encoder.transform(song_df['song_id'])\n\n# Display first 5 records of song_df data\nsong_df.head()","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718343676385,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"HDQJssAoqGto","outputId":"f1b5b6d5-fe4c-4687-ee16-65fdb8d2d6e1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It:** As the data also contains users who have listened to very few songs and vice versa, is it required to filter the data so that it contains users who have listened to a good count of songs and vice versa?","metadata":{"id":"6Q9EFYwj35Ju"}},{"cell_type":"markdown","source":"A dataset of size 2000000 rows x 7 columns can be quite large and may require a lot of computing resources to process. This can lead to long processing times and can make it difficult to train and evaluate your model efficiently.\nIn order to address this issue, it may be necessary to trim down your dataset to a more manageable size.","metadata":{"id":"gcY5LKAQvk9J"}},{"cell_type":"markdown","source":"**Locate the high-engagement users.**\n","metadata":{"id":"VC6cfQz8guiG"}},{"cell_type":"code","source":"# Get the column containing the users\nusers = df.user_id\n\n# Create a dictionary that maps users(listeners) to the number of songs that they have listened to\nplaying_count = dict()\n\nfor user in users:\n    # If we already have the user, just add 1 to their playing count\n    if user in playing_count:\n        playing_count[user] += 1\n\n    # Otherwise, set their playing count to 1\n    else:\n        playing_count[user] = 1","metadata":{"id":"7GGH9TW0_9uX","executionInfo":{"status":"ok","timestamp":1718343677306,"user_tz":240,"elapsed":925,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the song count distribution among users\nuser_song_count = pd.DataFrame(list(playing_count.items()), columns=['user_id', 'song_count'])\n\nplt.figure(figsize=(12, 6))\nsns.histplot(data=user_song_count, x='song_count', binwidth=1)\nplt.locator_params(axis='x', nbins=15)\nplt.xlabel('Song Count', fontsize=10)\nplt.ylabel('No. of Users', fontsize=10)\nplt.xticks(fontsize=8, rotation=90)\n# Set x-ticks at specific intervals\nax = plt.gca()\nax.set_xticks(np.arange(0, 800, 10))\nplt.yticks(fontsize=8)\nax.set_yticks(np.arange(0, 4000, 100))\nplt.show()","metadata":{"executionInfo":{"elapsed":1884,"status":"ok","timestamp":1718343679189,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"t1AcxPm8YmW3","outputId":"ee7e68fb-27c9-4b59-bcbb-d1130bed8302"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the cutoff point for users\nuser_song_count.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718343679189,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"Nn1cCCjdZ886","outputId":"ce430289-791c-45e3-8491-65f39f7c2e7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*   **Choose 90 songs as cut off point. They're the top 5% users who demonstrate higher music consumption.**\n","metadata":{"id":"euXD8qySkoSY"}},{"cell_type":"code","source":"# We want our users to have listened at least 90 songs\nSONG_COUNT_CUTOFF = 90\n\n# Create a list of users who need to be removed\nremove_users = []\n\nfor user, num_songs in playing_count.items():\n\n    if num_songs < SONG_COUNT_CUTOFF:\n        remove_users.append(user)\n\ndf = df.loc[ ~ df.user_id.isin(remove_users)]","metadata":{"id":"-cc6mOK7_9uX","executionInfo":{"status":"ok","timestamp":1718343679189,"user_tz":240,"elapsed":6,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Locate the popular songs.**\n","metadata":{"id":"FdvyCCJ5hIeb"}},{"cell_type":"code","source":"# Get the column containing the songs\nsongs = df.song_id\n\n# Create a dictionary that maps songs to its number of users(listeners)\nplaying_count = dict()\n\nfor song in songs:\n    # If we already have the song, just add 1 to their playing count\n    if song in playing_count:\n        playing_count[song] += 1\n\n    # Otherwise, set their playing count to 1\n    else:\n        playing_count[song] = 1","metadata":{"id":"B5BS-Wk5_9uY","executionInfo":{"status":"ok","timestamp":1718343679958,"user_tz":240,"elapsed":775,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the cutoff point for songs\nsong_user_count = pd.DataFrame(list(playing_count.items()), columns=['song_id', 'user_count'])\nsong_user_count.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"sDqnm8ChgOtZ","outputId":"ed622d99-94bc-44aa-a0a1-a6743a66404f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Choose 120 users as cut off point. It will cover the top 50% songs.**","metadata":{"id":"NWXE2dBXnIA8"}},{"cell_type":"code","source":"# We want our song to be listened by atleast 120 users to be considred\nLISTENER_COUNT_CUTOFF = 120\n\nremove_songs = []\n\nfor song, num_users in playing_count.items():\n    if num_users < LISTENER_COUNT_CUTOFF:\n        remove_songs.append(song)\n\ndf_final= df.loc[ ~ df.song_id.isin(remove_songs)]","metadata":{"id":"_nCtGwGO_9uY","executionInfo":{"status":"ok","timestamp":1718343679958,"user_tz":240,"elapsed":17,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check df_final record numbers\ndf_final.info()","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"CqAk3VRZW8LP","outputId":"ce29c198-b377-42dc-c703-10216073df5c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we do not have ratings data of the songs. In this case, we are going to use play_count as a proxy for ratings with the assumption that the more the user listens to a song, the higher the chance that they like the song. We need to check the play_count values distribution.**","metadata":{"id":"PzGKRYodpevT"}},{"cell_type":"code","source":"# Find the distribution of play_count values\ndf_final['play_count'].describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95])","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"iKM3hsCfi94D","outputId":"a94e8cfe-3d5a-4690-ba81-cb68b9f65596"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of all the songs available, songs with play_count less than or equal to 5 are in almost 90% abundance. So for building the recommendation system let us consider only those songs. The outliers, such as users who have listened to a song an excessively high number of times, can skew the analysis.","metadata":{"id":"VKkT8kwHyEpT"}},{"cell_type":"code","source":"# Keep only records of songs with play_count less than or equal to (<=) 5\ndf_final = df_final[df_final.play_count<=5]","metadata":{"id":"8qaKeoMcGpad","executionInfo":{"status":"ok","timestamp":1718343679958,"user_tz":240,"elapsed":13,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the data\ndf_final.shape","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"aL1JZ00o5JtQ","outputId":"5da317a4-855b-4fae-a2f9-ff63956f3aad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove those song_id records which do not appear in df_final\nsong_df = song_df[song_df['song_id'].isin(df_final['song_id'])]\n\n# Check song_df info afterwards\nsong_df.info()","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"XDVX64TS6F0A","outputId":"7419ce59-838a-4ae7-9ceb-e3f654a57a20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Exploratory Data Analysis**","metadata":{"id":"uZcr1Eke2T9W"}},{"cell_type":"markdown","source":"### **Let's check the total number of unique users, songs, artists in the data**","metadata":{"id":"ByuHmvWDeBJI"}},{"cell_type":"markdown","source":"Total number of unique user id","metadata":{"id":"DE_gukSJ2T9W"}},{"cell_type":"code","source":"# Display total number of unique user_id\ndf_final['user_id'].nunique()","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"n5E24_Ec2T9W","outputId":"702d1339-9661-4bf1-dab1-64292cd41694"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total number of unique song id","metadata":{"id":"wV3BOTdJII-t"}},{"cell_type":"code","source":"# Display total number of unique song_id\ndf_final['song_id'].nunique()","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"5SlpPkIE2T9W","outputId":"384024cd-03c7-4bea-aff8-9dda05311a47"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total number of unique artists","metadata":{"id":"eGXPsCjXVpUW"}},{"cell_type":"code","source":"# Display total number of unique artists\ndf_final['artist_name'].nunique()","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"qSVUwb8h2T9X","outputId":"ba29bcfa-c02e-4df5-e124-f31328867f67"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Observations and Insights:**\n\n\n**We trim down the dataset to only consider:**\n\n1.   users have listened to at least 90 songs (high-engagement user)\n2.   songs have been listened by at least 120 users (high-quality music)\n3.   play count less than or equal to 5 (excluding outliers which can skew the analysis)\n\n\nAfter the trimming process, the dataset has been condensed to encompass **110,000+ user interactions** involving **560+ songs, 230+ artists and 3000+ users**. This refined dataset provides a more manageable and computationally efficient foundation for constructing recommendation systems.\n\nBased on the count of unique users and songs, there exists a potential for 1,776,265 interactions (3155 users * 563 songs) within the dataset. However, the actual recorded interactions amount to only 117,876, representing approximately **6.6%** of the total potential interactions. This discrepancy underscores the reality that not every user has engaged with every song in the dataset, a common occurrence in such datasets.\n\nWhile this presents an opportunity to develop recommendation systems for suggesting songs that users have yet to interact with, it also introduces **the challenge of data sparsity**. Addressing this challenge will be paramount in ensuring the efficacy and relevance of the recommendation algorithms.","metadata":{"id":"bvk-YAo-eGGW"}},{"cell_type":"markdown","source":"### **Let's find out about the most interacted songs and interacted users**","metadata":{"id":"rLdIfv22ISBK"}},{"cell_type":"markdown","source":"Most interacted songs","metadata":{"id":"W3DyN_8atsCx"}},{"cell_type":"code","source":"# Counting the number of users who have listened to a certain song\nsong_user_count = pd.DataFrame(df_final['song_id'].value_counts())\n\n# Merge with song_df to show the most interacted songs' info\nsong_user_count_with_title = pd.merge(song_user_count,song_df, on = 'song_id', how = 'left')\nsong_user_count_with_title","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718343679958,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"JxGieL1ZiNH1","outputId":"f754f2e0-454b-4022-d26e-10cc5a873736"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting distributions of play_count for 751 interactions with song_id 8582\n\n# Let us fix the size of the figure\nplt.figure(figsize = (7, 7))\n\ndf_final[df_final['song_id'] == 8582]['play_count'].value_counts().plot(kind = 'bar')\n\n# This gives a label to the variable on the x-axis\nplt.xlabel('Play Count')\n\n# This gives a label to the variable on the y-axis\nplt.ylabel('Count')\n\n# This displays the plot\nplt.show()","metadata":{"executionInfo":{"elapsed":523,"status":"ok","timestamp":1718343680476,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"pGjNeLd6_TLI","outputId":"e720996e-0835-4091-80cb-31c182a17ed3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the user count distribution among songs\n\nplt.figure(figsize=(12, 6))\nsns.histplot(data=song_user_count, x='count')\nplt.xlabel('User Count', fontsize=10)\nplt.ylabel('No. of Songs', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1718343680477,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"88SnESdd9Ya5","outputId":"d5cfc69d-dc2c-4b64-c387-e3e7fc5dd7ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting distributions of play_count for all interactions with all song_id\n\n# Let us fix the size of the figure\nplt.figure(figsize = (7, 7))\n\ndf_final['play_count'].value_counts().plot(kind = 'bar')\n\n# This gives a label to the variable on the x-axis\nplt.xlabel('Play Count')\n\n# This gives a label to the variable on the y-axis\nplt.ylabel('Count')\n\n# This displays the plot\nplt.show()","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718343680477,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"JjaCqIZvSrMh","outputId":"e430d70d-7a37-4507-85dd-e2f1217d504b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most interacted users","metadata":{"id":"nnoXCc9zIV45"}},{"cell_type":"code","source":"# Counting the number of songs which have been listened by a certain user to show the most interacted users\nuser_song_count = pd.DataFrame(df_final['user_id'].value_counts())\nuser_song_count","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718343680477,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"LcBYcUgo1LL9","outputId":"2cd5bedf-6657-4c59-f01b-c9903dd99eaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the song count distribution among users\n\nplt.figure(figsize=(12, 6))\nsns.histplot(data=user_song_count, x='count')\nplt.xlabel('Song Count', fontsize=10)\nplt.ylabel('No. of Users', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1718343680477,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"7-Q_xi718Skv","outputId":"f6c3adb3-8532-464c-e880-1a2f499b94ab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Observations and Insights:**\n\n*   The most interacted song has 751 users.\n*   The most interacted user listened to 243 songs.\n*   Among the 110,000+ interactions, the majorities' play count fall between 1 and 2. Over 70000 have play count as 1, Over 20000 have play count as 2, only around 5000 have play count as 5.  \n*   No matter songs, users or interaction play counts, they all follow the power law, ie. a small number of items (songs, users, or interaction play counts) account for the majority of the occurrences, while the majority of items have relatively low frequencies. For example, a few popular songs have a lot of interactions, while the majority of songs have a lot fewer.\n\n\n*   It underscores the complexities associated with data sparsity and the importance of mitigating potential biases inherent in the recommendation algorithm.\n\n","metadata":{"id":"tPZRc1e-eyyO"}},{"cell_type":"markdown","source":"Songs released on yearly basis","metadata":{"id":"joFF5zndX1Dk"}},{"cell_type":"code","source":"# Find out the number of songs released in a year, use the songs_df\n  # Hint: Use groupby function on the 'year' column\n\n  # Group by 'year' and count occurrences of 'song_id'\nyearly_song_counts = song_df.groupby(by='year')['song_id'].count()\n\n# Convert the Series to a DataFrame and reset the index\nyearly_song_counts = yearly_song_counts.reset_index()\n\n# Rename the columns for clarity\nyearly_song_counts.columns = ['year', 'count']\n\n# Filter out rows where 'year' is 0\nyearly_song_counts = yearly_song_counts[yearly_song_counts['year'] != 0]\n\n# Set the figure size\nplt.figure(figsize=(10, 6))\n\n# Create a barplot plot with y label as \"number of titles played\" and x -axis year\nplt.bar(yearly_song_counts['year'], yearly_song_counts['count'], color='skyblue',width=0.8)\n\n# Set the x label of the plot\nplt.xlabel('Year')\nplt.xticks(yearly_song_counts['year'],rotation=90)  # Ensure all years are shown on x-axis\n\n# Set the y label of the plot\nplt.ylabel('Number of Titles Played')\n\n# Show the plot\nplt.title('Number of Titles Played Per Year')\nplt.show()","metadata":{"executionInfo":{"elapsed":340,"status":"ok","timestamp":1718343680809,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"bQp2iVMC2T9Y","outputId":"82a91c61-1217-45dd-8ee3-2b90678cf9b7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Songs' user count by song release year","metadata":{"id":"k3fBdHo71Z1s"}},{"cell_type":"code","source":"# Group by year and count unique user_id\nuser_count_by_year = df_final.groupby('year')['user_id'].nunique().reset_index()\n# Filter out rows where 'year' is 0\nuser_count_by_year = user_count_by_year[user_count_by_year['year'] != 0]","metadata":{"id":"QPIjkrvL0a7z","executionInfo":{"status":"ok","timestamp":1718343680809,"user_tz":240,"elapsed":5,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the barplot\nplt.figure(figsize=(10, 5))\nplt.bar(user_count_by_year['year'], user_count_by_year['user_id'], color='skyblue')\nplt.xlabel('Year')\nplt.xticks(user_count_by_year['year'],rotation=90)  # Ensure all years are shown on x-axis\nplt.ylabel('Count of Unique Users')\nplt.title('Count of Unique Users by Year')\nplt.xticks(user_count_by_year['year'])\nplt.show()","metadata":{"id":"mAN_p2Mn0c3R","executionInfo":{"status":"ok","timestamp":1718343681677,"user_tz":240,"elapsed":873,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"9b406f65-b65b-4848-abbf-3e88ead69e6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Songs' total play count by song release year","metadata":{"id":"jF95_bdVzFQo"}},{"cell_type":"code","source":"song_year_play_count = df_final.groupby('year')['play_count'].sum().reset_index()\n# Filter out rows where 'year' is 0\nsong_year_play_count = song_year_play_count[song_year_play_count['year'] != 0]","metadata":{"id":"9668JhlJwfOk","executionInfo":{"status":"ok","timestamp":1718343681679,"user_tz":240,"elapsed":33,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the barplot\nplt.figure(figsize=(10, 5))\nplt.bar(song_year_play_count['year'], song_year_play_count['play_count'], color='skyblue')\nplt.xlabel('Song Year')\nplt.xticks(song_year_play_count['year'],rotation=90)  # Ensure all years are shown on x-axis\nplt.ylabel('Total Play Count')\nplt.title('Total Play Count by Song Year')\nplt.show()","metadata":{"id":"wNnWKjIQwW0c","executionInfo":{"status":"ok","timestamp":1718343681679,"user_tz":240,"elapsed":32,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"edf28a34-05f0-485d-964b-ecba12f4e809"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Artists having most songs","metadata":{"id":"hl9NX4bovbnw"}},{"cell_type":"code","source":"# Counting the number of songs for each artist\nsong_df['artist_name'].value_counts()","metadata":{"id":"fvuBODRgvbnw","outputId":"d033b18e-feee-4acf-841d-4432ca9724ab","executionInfo":{"status":"ok","timestamp":1718343681679,"user_tz":240,"elapsed":31,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# song artist analysis\n\n# Group by 'artist_name' and count occurrences of 'song_id'\nartist_song_counts = song_df.groupby(by='artist_name')['song_id'].count()\n\n# Convert the Series to a DataFrame and reset the index\nartist_song_counts = artist_song_counts.reset_index()\n\n# Rename the columns for clarity\nartist_song_counts.columns = ['artist_name', 'song_count']\n\n# Set the figure size\nplt.figure(figsize=(12, 10))\n\n# Create a histplot with y label as \"Number of Artists\" and x -axis Song Count\n\nsns.histplot(data=artist_song_counts, x='song_count', binwidth=1)\nplt.locator_params(axis='x', nbins=25)\n\n# Set the x label of the plot\nplt.xlabel('Song Count')\n\n# Set the y label of the plot\nplt.ylabel('Number of Artists')\n\n# Show the plot\nplt.title('Song Count Per Artist')\nplt.show()","metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1718343682057,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"Kfm-PDNFgD4d","outputId":"a5177fe9-b2e4-4874-e6ee-7f19307b44af"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Artists having most users","metadata":{"id":"eaupEvMn53Wy"}},{"cell_type":"code","source":"user_count_by_artist = df_final.groupby('artist_name')['user_id'].nunique().reset_index()\nuser_count_by_artist = user_count_by_artist.sort_values(by='user_id', ascending=False)\nuser_count_by_artist\n","metadata":{"id":"lPRR2iXZ2kqP","executionInfo":{"status":"ok","timestamp":1718343682057,"user_tz":240,"elapsed":5,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"381b5abf-e948-44ee-f9ea-d9eaefe3baf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.bar(user_count_by_artist['artist_name'], user_count_by_artist['user_id'], color='skyblue')\nplt.xlabel('Artist Name')\nplt.ylabel('Count of Unique Users')\nplt.title('Count of Unique Users by Artist Name')\nplt.xticks(rotation=90, ha='right', fontsize=5)\nplt.tight_layout()  # Adjust layout to make room for rotated labels\nplt.gca().invert_yaxis()  # Invert y-axis to have the largest bar at the top\nplt.show()","metadata":{"id":"qdWC682z2_Yx","executionInfo":{"status":"ok","timestamp":1718343684367,"user_tz":240,"elapsed":2314,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"85dcc863-b0b7-4a16-f563-89bf005ef3ae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Artists having most play count","metadata":{"id":"hXyuSKVQOZoA"}},{"cell_type":"code","source":"play_count_by_artist = df_final.groupby('artist_name')['play_count'].sum().reset_index()\nplay_count_by_artist = play_count_by_artist.sort_values(by='play_count', ascending=False)\nplay_count_by_artist\n","metadata":{"id":"Vr9uciqq6R_V","executionInfo":{"status":"ok","timestamp":1718343684367,"user_tz":240,"elapsed":17,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"57e3528d-c3cc-4e32-ba2a-3d6c38dfc5db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the barplot\nplt.figure(figsize=(15, 10))\nplt.bar(play_count_by_artist['artist_name'], play_count_by_artist['play_count'], color='skyblue')\nplt.xlabel('Artist Name')\nplt.xticks(play_count_by_artist['artist_name'],rotation=90)  # Ensure all artist name are shown on x-axis\nplt.ylabel('Total Play Count')\nplt.title('Total Play Count by Artist')\nplt.xticks(rotation=90, ha='right', fontsize=5)\nplt.tight_layout()  # Adjust layout to make room for rotated labels\nplt.gca().invert_yaxis()  # Invert y-axis to have the largest bar at the top\nplt.show()","metadata":{"id":"zZCujjIzOOP5","executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":2167,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"b8ae7506-2f75-4368-c349-699a257c3f07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Release having most play count","metadata":{"id":"1-azqwxiOg-y"}},{"cell_type":"code","source":"release_play_count = df_final.groupby('release')['play_count'].sum().reset_index()\nrelease_play_count = release_play_count.sort_values(by='play_count', ascending=False)\nrelease_play_count","metadata":{"id":"xFesJjzN6SPu","executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":20,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"47ead11f-bd68-4aa0-cb48-02578a6894c5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Release having most users","metadata":{"id":"3HurOsciOnpz"}},{"cell_type":"code","source":"user_count_by_release = df_final.groupby('release')['user_id'].nunique().reset_index()\nuser_count_by_release = user_count_by_release.sort_values(by='user_id', ascending=False)\nuser_count_by_release\n","metadata":{"executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":18,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"0578c1cc-f9cd-4b77-ec82-8e1f483259f7","id":"c18T4b6ESeco"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Release having most songs (remember in our dataset these songs are already been selected as more popular ones)","metadata":{"id":"J_6OuQUmOrja"}},{"cell_type":"code","source":"release_song_count = song_df.groupby('release')['song_id'].count().reset_index()\nrelease_song_count = release_song_count.sort_values(by='song_id', ascending=False)\nrelease_song_count","metadata":{"executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":18,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"18ed9b1c-0400-41a4-b645-c62deda0d2df","id":"Bx7BjKolRicD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Years having most release","metadata":{"id":"-jzI74dUPM56"}},{"cell_type":"code","source":"release_year_count = song_df.groupby('year')['release'].nunique().reset_index()\nrelease_year_count = release_year_count[release_year_count['year'] != 0].sort_values(by='release', ascending=False)\nrelease_year_count","metadata":{"executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":17,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"fd49496d-2aa0-4ec6-d4a6-de53e968f631","id":"h15mC-zHTtTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Artists having most release","metadata":{"id":"GA9eYdI5PVlQ"}},{"cell_type":"code","source":"artist_release_count = song_df.groupby('artist_name')['release'].nunique().reset_index()\nartist_release_count = artist_release_count.sort_values(by='release', ascending=False)\nartist_release_count\n\n","metadata":{"executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":17,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"42634655-a7b7-474e-eda2-e4b9b5001d2a","id":"Povk4fVyVEhw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Observations and Insights:** #\n\n\n\n*   The Million Song Dataset released in 2011. That explains most of the songs in the dataset comes from 2000-2010.\n*   The artists and their no. of songs follow the power law too. A few popular artists have more songs, while the majority only have relatively low no. of songs.  \n*   We do further analysis to find the popular artists and releases.\n\n","metadata":{"id":"VUcXc7ZYfaGl"}},{"cell_type":"markdown","source":"**Think About It:** What other insights can be drawn using exploratory data analysis?\n\nIf have more time, we can consider to do further analysis on play count distribution among artists.","metadata":{"id":"RtAjyDMioHCp"}},{"cell_type":"markdown","source":"Now that we have explored the data, let's apply different algorithms to build recommendation systems.\n\n**Note:** Use the shorter version of the data, i.e., the data after the cutoffs as used in Milestone 1.","metadata":{"id":"OWO4C8KsK_5e"}},{"cell_type":"markdown","source":"## Building various models","metadata":{"id":"9VThYg7voGIz"}},{"cell_type":"markdown","source":"### **Popularity-Based Recommendation Systems**","metadata":{"id":"Ituk9wA4Idib"}},{"cell_type":"markdown","source":"Let's take the count and sum of play counts of the songs and build the popularity recommendation systems based on the sum of play counts.","metadata":{"id":"462hsbxaI1ED"}},{"cell_type":"code","source":"# Calculating average play_count\n       # Hint: Use groupby function on the song_id column\naverage_count = df_final.groupby('song_id')['play_count'].mean()\n\n# Calculating the frequency a song is played\n      # Hint: Use groupby function on the song_id column\nplay_freq = df_final.groupby('song_id')['play_count'].count()","metadata":{"id":"UXhBZlDE-jEu","executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":16,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a dataframe with the average_count and play_freq\nfinal_play = pd.DataFrame({'average_count': average_count, 'play_freq': play_freq})\n\n# Let us see the first five records of the final_play dataset\nfinal_play.head()","metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1718343686519,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"v2XYdXvWdyys","outputId":"46dbdec8-111a-468e-f7d6-ee394089dbdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's create a function to find the top n songs for a recommendation based on the average play count of song. We can also add a threshold for a minimum number of playcounts for a song to be considered for recommendation.","metadata":{"id":"WnCT-A7RK_5g"}},{"cell_type":"code","source":"# Build the function to find top n songs based on the highest average rating and minimum interactions\ndef top_n_songs(data, n, min_interaction = 100):\n\n# Finding products with minimum number of interactions\n    recommendations = data[data['play_freq'] > min_interaction]\n\n# Sorting values with respect to average play\n    recommendations = data.sort_values(by = 'average_count', ascending = False)\n\n    return recommendations.index[:n]","metadata":{"id":"QiT9FV3GNCrb","executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":15,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recommend top 10 songs using the function defined above\nlist(top_n_songs(final_play, 10))","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1718343686519,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"GpZt_BeXgz4F","outputId":"e09de790-e807-4076-fc39-163975a2b37a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's define a data frame to save the models' metrics for later comparision.**","metadata":{"id":"eM7Uk40Yixdf"}},{"cell_type":"code","source":"# Define the columns for the DataFrame to save model's info\ndf_model_info_columns = ['Model Type', 'Parameters', 'Precision', 'Recall', 'F1', 'RMSE', 'Combination']\n\n# Create an empty DataFrame with the specified columns\ndf_model_info = pd.DataFrame(columns=df_model_info_columns)\n\n# Function to add a new model record to the DataFrame\ndef add_model_record(df, model_type, parameters, precision, recall, f1, rmse, model_combination=''):\n    new_record = {\n        'Model Type': model_type,\n        'Parameters': parameters,\n        'Precision': precision,\n        'Recall': recall,\n        'F1': f1,\n        'RMSE': rmse,\n        'Combination': model_combination\n    }\n    df.loc[len(df)] = new_record","metadata":{"id":"AEvx1B4Dix0A","executionInfo":{"status":"ok","timestamp":1718343686519,"user_tz":240,"elapsed":11,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **User User Similarity-Based Collaborative Filtering**","metadata":{"id":"gf13HrPPJeWT"}},{"cell_type":"markdown","source":"To build the user-user-similarity-based and subsequent models we will use the \"surprise\" library.","metadata":{"id":"ROcEpduohdua"}},{"cell_type":"code","source":"# Install the surprise package using pip. Uncomment and run the below code to do the same\n\n!pip install surprise","metadata":{"executionInfo":{"elapsed":5398,"status":"ok","timestamp":1718343691906,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"aKLrKn8IfGjk","outputId":"9949a9de-e738-440a-a4e0-a2186ce66c9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\n\n# To compute the accuracy of models\nfrom surprise import accuracy\n\n# This class is used to parse a file containing play_counts, data should be in structure - user; item; play_count\nfrom surprise.reader import Reader\n\n# Class for loading datasets\nfrom surprise.dataset import Dataset\n\n# For tuning model hyperparameters\nfrom surprise.model_selection import GridSearchCV\n\n# For splitting the data in train and test dataset\nfrom surprise.model_selection import train_test_split\n\n# For implementing similarity-based recommendation system\nfrom surprise.prediction_algorithms.knns import KNNBasic\n\n# For implementing matrix factorization based recommendation system\nfrom surprise.prediction_algorithms.matrix_factorization import SVD\n\n# For implementing KFold cross-validation\nfrom surprise.model_selection import KFold\n\n# For implementing clustering-based recommendation system\nfrom surprise import CoClustering","metadata":{"id":"UJ1wEylUpexj","executionInfo":{"status":"ok","timestamp":1718343691907,"user_tz":240,"elapsed":13,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some useful functions","metadata":{"id":"jBW4BUhWTsnm"}},{"cell_type":"markdown","source":"Below is the function to calculate precision@k and recall@k, RMSE, and F1_Score@k to evaluate the model performance.","metadata":{"id":"ZhFa_4aHHchr"}},{"cell_type":"markdown","source":"**Think About It:** Which metric should be used for this problem to compare different models?\n\n\n\n*   Precision is the proportion of recommended items that are relevant. Higher precision will have better user experience. If users have to sift through many irrelevant recommendations, their satisfaction may decrease.\n*   Recall is the proportion of relevant items that are recommended. Higher recall will help users discover as much new and relevant content as possible. It can lead to more engagement, as users find more songs that they enjoy. This is particularly important in competitive markets where retaining user attention is crucial.\n*   F1-score is the harmonic mean of precision and recall, providing a single metric that balances the two. Its particularly useful when both precision and recall are important and a trade-off needs to be considered.\n  \n\n","metadata":{"id":"ZOvOgjGWrMVV"}},{"cell_type":"code","source":"def precision_recall_at_k(model, k=30, threshold=1.5):\n    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n\n    #Making predictions on the test data\n    predictions = model.test(testset)\n\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, playing_count in user_est_true.items():\n\n        # Sort play count by estimated value\n        playing_count.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in playing_count)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in playing_count[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in playing_count[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        # When n_rec_k is 0, Precision is undefined. We here set Precision to 0 when n_rec_k is 0.\n\n        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n\n        # Recall@K: Proportion of relevant items that are recommended\n        # When n_rel is 0, Recall is undefined. We here set Recall to 0 when n_rel is 0.\n\n        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n\n    #Mean of all the predicted precisions are calculated.\n    precision = round((sum(prec for prec in precisions.values()) / len(precisions)),8)\n    #Mean of all the predicted recalls are calculated.\n    recall = round((sum(rec for rec in recalls.values()) / len(recalls)),8)\n    # Formula to compute the F-1 score.\n    f1 = round((2*precision*recall)/(precision+recall),8)\n    # Retrieve RMSE and print\n    rmse = round(accuracy.rmse(predictions),8)\n\n    print('Precision: ', precision) #Command to print the overall precision\n    print('Recall: ', recall) #Command to print the overall recall\n    print('F_1 score: ', f1) #Command to print the F-1 score\n\n    # Return all 4 scores\n    return precision, recall, f1, rmse","metadata":{"id":"Rxn-GahOTsnm","executionInfo":{"status":"ok","timestamp":1718343691907,"user_tz":240,"elapsed":12,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It:** In the function precision_recall_at_k above the threshold value used is 1.5. How precision and recall are affected by changing the threshold? What is the intuition behind using the threshold value of 1.5?\n\n\n\n*   Precision: Precision measures the proportion of recommended items that are relevant to the user out of all recommended items. When the threshold is increased, it becomes harder for items to be considered relevant, leading to potentially fewer true positives (relevant items) being recommended. However, because the number of false positives (irrelevant items) decreases more significantly, the precision typically increases as the threshold increases, making the algorithm more conservative in its recommendations.\n\n*   Recall: Recall measures the proportion of relevant items that are successfully recommended out of all relevant items in the dataset. Increasing the threshold causes the algorithm to be more selective in recommending items, potentially missing some relevant items that would have been recommended with a lower threshold. This leads to an increase in false negatives and consequently a decrease in recall as the threshold increases, because the algorithm may overlook some relevant items.\n\n*   The selection of the threshold value, specifically 1.5 in our case, corresponds to the play count criterion within our dataset. In above data analysis, the majority fall between 1 and 2, a threshold of 1.5 helps filter out items that are not strongly relevant while keeping those that are most likely to be of interest. Given that this threshold represents a play count above which a song is considered significant (rounded to 2 for practical application), it signifies instances where a user has played a song more than once. The rationale behind this choice stems from the recognition of our dataset's sparsity, necessitating a cautious balance in recommendation strategies. Opting for a threshold lower than 1.5 could potentially result in an overabundance of irrelevant song recommendations, which would compromise the effectiveness of our recommendation system.\n\n","metadata":{"id":"PcmLRxH4IjfG"}},{"cell_type":"markdown","source":"Below we are loading the **dataset**, which is a **pandas dataframe**, into a **different format called `surprise.dataset.DatasetAutoFolds`** which is required by this library. To do this we will be **using the classes `Reader` and `Dataset`**\n\nYou will also notice here that we read the dataset by providing a scale of ratings. However, as you would know, we do not have ratings data of the songs. In this case, we are going to use play_count as a proxy for ratings with the assumption that the more the user listens to a song, the higher the chance that they like the song.","metadata":{"id":"LvvoxESjyEpY"}},{"cell_type":"code","source":"# Instantiating Reader scale with expected rating scale\n #use rating scale (0, 5)\nreader = Reader(rating_scale = (0, 5))\n\n# Loading the dataset\n # Take only \"user_id\",\"song_id\", and \"play_count\"\ndata = Dataset.load_from_df(df_final[['user_id', 'song_id', 'play_count']], reader)\n\n# Splitting the data into train and test dataset\n # Take test_size = 0.4, random_state = 42\ntrainset, testset = train_test_split(data, test_size = 0.4, random_state = 42)","metadata":{"id":"rGfYDiOCpe4X","executionInfo":{"status":"ok","timestamp":1718343692646,"user_tz":240,"elapsed":751,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It:** How changing the test size would change the results and outputs?\n\n\n\n*   As the test size increases, the proportion of data available for training decreases. This could potentially impact the model's ability to learn from the training data, especially if the training set becomes too small. With a smaller training set, the model may not capture the underlying patterns in the data as effectively, which could lead to poorer performance.\n\n*   But a larger test size means more data is held out for evaluation, providing a more reliable estimate of how well the model will perform on unseen data.\n\n*   A smaller test size may lead to higher variance because the evaluation of model performance is more sensitive to the specific random split of the data. Conversely, a larger test size may lead to higher bias because the model has less data for training and may generalize less effectively to unseen data.\n\nIn summary, changing the test size when splitting the data can impact the trade-off between training and testing performance, generalization ability, bias and variance, and the robustness of the results.\n","metadata":{"id":"CuTmLjUP1aED"}},{"cell_type":"code","source":"# Build the default user-user-similarity model\nsim_options = {'name': 'cosine',\n               'user_based': True}\n\n# KNN algorithm is used to find desired similar items\n # Use random_state = 1\nsim_user_user = KNNBasic(sim_options = sim_options, verbose = False, random_state = 1)\n\n# Train the algorithm on the trainset, and predict play_count for the testset\nsim_user_user.fit(trainset)","metadata":{"executionInfo":{"elapsed":763,"status":"ok","timestamp":1718343694009,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"vO3FL7iape8A","outputId":"a21a6757-eadc-468f-a264-b0a732836000"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k, recall@k, and f_1 score with k = 30\n# Use sim_user_user model\nprecision, recall, f1, rmse = precision_recall_at_k(sim_user_user)\n# Save the model's info\nsim_user_user_params = f\"k={sim_user_user.k}, min_k={sim_user_user.min_k}, sim_options={sim_user_user.sim_options}, verbose={sim_user_user.verbose}\"\nadd_model_record(df_model_info, 'User-User', sim_user_user_params, precision, recall, f1, rmse)","metadata":{"id":"DKQXFgpNrWbX","executionInfo":{"status":"ok","timestamp":1718343704270,"user_tz":240,"elapsed":10264,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"b4f67dd1-4817-4608-bdf5-2af58e082205"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model info dataframe\ndf_model_info","metadata":{"id":"zn0WNJCcsXZ_","executionInfo":{"status":"ok","timestamp":1718343704270,"user_tz":240,"elapsed":23,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"25c90c22-87bb-4926-bb92-57ce7f413791"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n\n- We have calculated **RMSE** to check **how far the overall predicted play counts** are from the **actual play counts**.\n- Intuition of Recall: We are getting a **recall of ~0.692**, which means out of **all the relevant songs 69.2% are recommended**.\n- Intuition of Precision: We are getting a **precision of ~ 0.396**, which means **out of all the recommended songs 39.6% are relevant**.\n- Here **F_1 score** of the **baseline model is ~0.504**. It indicates that **the model is performing moderately**, precision and recall are balanced. The model is reasonably good at minimizing false positives (precision) and identifying relevant instances (recall) but does not excel in either aspect.\n- We will try to improve this later by using **GridSearchCV by tuning different hyperparameters** of this algorithm.\n- We create a dataframe to save the model's info for later comparision.\n\n","metadata":{"id":"mzcdlWmer6GA"}},{"cell_type":"code","source":"# Predicting play_count for a sample user with a listened song\n# Use any user id  and song_id\nsim_user_user.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1718343704270,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"Sxd23bZ9pe_x","outputId":"a4aaa883-a56d-4fb9-8122-6729c97a11c6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting play_count for a sample user with a song not-listened by the user\n #predict play_count for any sample user\nsim_user_user.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718343704270,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"PbFcBj1PpfEV","outputId":"459c9e4d-b47b-4611-c9b8-7c95b0c45f82"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n- The above output shows that **the actual play count 2 is close to the predicted play count 1.8 for this user-item pair** by this **user-user-similarity-based baseline model**.\n- The **output** also contains **\"actual_k\"**. It is the value of **K in KNN** that is used while training the model. The default value is 40.\n- Above, we also **predicted the play count for a non-interacted user-item pair** based on this user-user similarity-based baseline model.\n\n","metadata":{"id":"P9EVM7DysC47"}},{"cell_type":"markdown","source":"Now, let's try to tune the model and see if we can improve the model performance.","metadata":{"id":"Lt1QBiylsIOm"}},{"cell_type":"code","source":"# Setting up parameter grid to tune the hyperparameters\nparam_grid = {'k': [30, 40, 50], 'min_k': [3, 6, 9],\n              'sim_options': {'name': ['msd', 'cosine'],\n                              'user_based': [True]}\n              }\n\n# Performing 3-fold cross-validation to tune the hyperparameters\ngs = GridSearchCV(KNNBasic, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)\n\n# Fitting the data\n # Use entire data for GridSearch\ngs.fit(data)\n\n# Best RMSE score\nprint(gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","metadata":{"id":"T3diJPL7-tVw","outputId":"cf210bc9-8efe-4163-cb5d-4e3f865d4de6","executionInfo":{"status":"ok","timestamp":1718343800758,"user_tz":240,"elapsed":96504,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the best model found in above gridsearch\n\n# Using the optimal similarity measure for user-user based collaborative filtering\nsim_options = {'name': 'msd',\n               'user_based': True}\n\n# Creating an instance of KNNBasic with optimal hyperparameter values\nsim_user_user_optimized = KNNBasic(sim_options = sim_options, k = 50, min_k = 9, random_state = 1, verbose = False)\n\n# Training the algorithm on the trainset\nsim_user_user_optimized.fit(trainset)","metadata":{"executionInfo":{"elapsed":1365,"status":"ok","timestamp":1718343802097,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"PujRJA8X_JEJ","outputId":"68a0215a-e387-40ac-810e-ac51afe7d1ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(sim_user_user_optimized)\n\n# Save the model's info\nsim_user_user_optimized_params = f\"k={sim_user_user_optimized.k}, min_k={sim_user_user_optimized.min_k}, sim_options={sim_user_user_optimized.sim_options}, verbose={sim_user_user_optimized.verbose}\"\nadd_model_record(df_model_info, 'User-User Optimized', sim_user_user_optimized_params, precision, recall, f1, rmse)","metadata":{"id":"esRtyoUUuBWh","executionInfo":{"status":"ok","timestamp":1718343811528,"user_tz":240,"elapsed":9435,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"8c569e98-0a2c-4eaf-9ad3-1fc520afcdcf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- We can see from above that after tuning hyperparameters, **the RMSE of the model has gone down as compared to the model before hyperparameter tuning**. Along with this, **Precision score of the tuned model has increased in comparison to the baseline model**, but recall and F1 score went down. The model performance didn't improve much after hyperparameter tuning.","metadata":{"id":"MH5OBZ7Nse6m"}},{"cell_type":"code","source":"# Predict the play count for a user who has listened to the song. Take user_id 6958, song_id 1671 and r_ui = 2\nsim_user_user_optimized.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1718343811529,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"FgV63lHiq1TV","outputId":"42b54f94-2b73-4841-c495-e7d69937e058"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the play count for a song that is not listened to by the user (with user_id 6958)\nsim_user_user_optimized.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1718343811529,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"HXO2Ztjhq1bN","outputId":"4dce0a1c-80a6-4954-9bfb-a3d90f09f800"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The predicted play count by the tuned model varies from the prediction by the baseline model, deviating notably from the actual play count.","metadata":{"id":"SdpJ--8QWuzz"}},{"cell_type":"markdown","source":"**Think About It:** Along with making predictions on listened and unknown songs can we get 5 nearest neighbors (most similar) to a certain song?","metadata":{"id":"SQ9M4pplNbWS"}},{"cell_type":"code","source":"# Use inner id 0\nsim_user_user_optimized.get_neighbors(0, k = 5)","metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1718343811529,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"TbFle7cKmBJG","outputId":"71899e00-e90b-4c73-a8aa-c3987ee94682"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we will be implementing a function where the input parameters are:\n\n- data: A **song** dataset\n- user_id: A user-id **against which we want the recommendations**\n- top_n: The **number of songs we want to recommend**\n- algo: The algorithm we want to use **for predicting the play_count**\n- The output of the function is a **set of top_n items** recommended for the given user_id based on the given algorithm","metadata":{"id":"U3ESobDynVNI"}},{"cell_type":"code","source":"def get_recommendations(data, user_id, top_n, algo):\n\n    # Creating an empty list to store the recommended song ids\n    recommendations = []\n\n    # Creating an user item interactions matrix\n    user_item_interactions_matrix = data.pivot(index = 'user_id', columns = 'song_id', values = 'play_count')\n\n    # Extracting those song ids which the user_id has not played yet\n    non_interacted_songs = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n\n    # Looping through each of the song ids which user_id has not interacted yet\n    for song_id in non_interacted_songs:\n\n        # Predicting the users for those non played song ids by this user\n        est = algo.predict(user_id, song_id).est\n\n        # Appending the predicted play_counts\n        recommendations.append((song_id, est))\n\n    # Sorting the predicted play_counts in descending order\n    recommendations.sort(key = lambda x: x[1], reverse = True)\n\n    return recommendations[:top_n] # Returing top n highest predicted play_count songs for this user","metadata":{"id":"vW9V1Tk65HlY","executionInfo":{"status":"ok","timestamp":1718343811529,"user_tz":240,"elapsed":26,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make top 5 recommendations for any user_id with a similarity-based recommendation engine\nrecommendations = get_recommendations(df_final, 6958, 5, sim_user_user_optimized)","metadata":{"id":"qWbR85mI5Hrk","executionInfo":{"status":"ok","timestamp":1718343811529,"user_tz":240,"elapsed":26,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the dataframe for above recommendations with columns \"song_id\" and \"predicted_play_count\"\npd.DataFrame(recommendations, columns = ['song_id', 'predicted_play_count'])","metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1718343811529,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"b5WfIX0Z6_q2","outputId":"a57c93bf-3eba-4de7-cd92-00d516f74b12"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n\n*   We implement a function to Make top n recommendations for any user id with a fitted model. It will return the song ids and predicted play counts.\n\n\n","metadata":{"id":"kyhThMOttWjj"}},{"cell_type":"markdown","source":"### Correcting the play_counts and Ranking the above songs","metadata":{"id":"ghwEJY2e7INB"}},{"cell_type":"code","source":"def ranking_songs(recommendations, final_play):\n  # Sort the songs based on play counts\n  ranked_songs = final_play.loc[[items[0] for items in recommendations]].sort_values('play_freq', ascending = False)[['play_freq']].reset_index()\n\n  # Merge with the recommended songs to get predicted play_counts\n  ranked_songs = ranked_songs.merge(pd.DataFrame(recommendations, columns = ['song_id', 'predicted_play_count']), on = 'song_id', how = 'inner')\n\n  # Rank the songs based on corrected play_counts\n  ranked_songs['corrected_play_count'] = ranked_songs['predicted_play_count'] - 1 / np.sqrt(ranked_songs['play_freq'])\n\n  # Sort the songs based on corrected play_counts\n  ranked_songs = ranked_songs.sort_values('corrected_play_count', ascending = False)\n\n  return ranked_songs","metadata":{"id":"39Hs7ZbO9v3O","executionInfo":{"status":"ok","timestamp":1718343811529,"user_tz":240,"elapsed":23,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It:** In the above function to correct the predicted play_count a quantity 1/np.sqrt(n) is subtracted. What is the intuition behind it? Is it also possible to add this quantity instead of subtracting?\n\n\n\n*   By subtracting 1/np.sqrt(n), the model is penalized slightly, which helps to prevent overfitting. When the number of observations is small, there's a higher risk of overfitting because the model may try to fit the noise in the data rather than capturing the underlying patterns. It can be seen as a form of bias correction. It helps to adjust the predictions to be more conservative, especially when dealing with limited data. And it makes the predictions more robust by reducing the influence of outliers or extreme values.\n\n*   It's possible to add instead of subtracting. Adding would have a similar effect of regularizing the predictions and biasing them slightly, but in the opposite direction. It will get more optimistic predictions.\n\n","metadata":{"id":"zQvst41lOoMX"}},{"cell_type":"code","source":"# Applying the ranking_songs function on the final_play data\nranking_songs(recommendations, final_play)","metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1718343811530,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"xoiAL_vH8miC","outputId":"872a4912-fbd5-4cfb-860b-863d802ad93f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n\n*   After the correction, the predicted play counts all decrease slightly. The magnitude of this decrease is proportional to the play frequency.\n\n","metadata":{"id":"KOwwGsH8toLG"}},{"cell_type":"markdown","source":"### Item Item Similarity-based collaborative filtering recommendation systems","metadata":{"id":"QgbzJKk7Tsnr"}},{"cell_type":"code","source":"# Apply the item-item similarity collaborative filtering model with random_state = 1 and evaluate the model performance\nsim_options = {'name': 'cosine',\n               'user_based': False}\n\n# The KNN algorithm is used to find desired similar items\nsim_item_item = KNNBasic(sim_options = sim_options, random_state = 1, verbose = False)\n\n# Train the algorithm on the train set, and predict ratings for the testset\nsim_item_item.fit(trainset)","metadata":{"executionInfo":{"elapsed":504,"status":"ok","timestamp":1718343812012,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"W5RMcdzjTsns","outputId":"d2774e31-2354-4973-9703-69340bbc35f6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(sim_item_item)\n\n# Save the model's info\nsim_item_item_params = f\"k={sim_item_item.k}, min_k={sim_item_item.min_k}, sim_options={sim_item_item.sim_options}, verbose={sim_item_item.verbose}\"\nadd_model_record(df_model_info, 'Item-Item', sim_item_item_params, precision, recall, f1, rmse)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343814681,"user_tz":240,"elapsed":2674,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"feff9f2a-fc43-45e2-9eee-d488041fa535","id":"2_did2mhudBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- Here, **F_1 score** of the **baseline model** is **~0.397**. We will try to improve this later by tuning different hyperparameters of this algorithm using **GridSearchCV**.","metadata":{"id":"JfdIJ6XWunx0"}},{"cell_type":"code","source":"# Predicting play count for a sample user_id 6958 and song (with song_id 1671) listened to by the user\nsim_item_item.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1718343814681,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"5yILOxXRTsns","outputId":"9869e0ac-5517-4d5c-9a79-5a174dd54617"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the play count for a user that has not listened to the song (with song_id 1671)\nsim_item_item.predict(67704, 1671, verbose = True)","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718343814682,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"jSn8oK3JZsTc","outputId":"ad75034f-b866-4c55-b00c-bd33f4775ede"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the play count for a song that is not listened to by the user (with user_id 6958)\nsim_item_item.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718343814682,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"pBCNac6QhGqc","outputId":"aff11bb6-63d8-4b4a-91bc-04013a924556"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The predicted play count by this model deviate notably from the actual play count.","metadata":{"id":"dxE9fJ8Dupby"}},{"cell_type":"code","source":"# Apply grid search for enhancing model performance\n\n# Setting up parameter grid to tune the hyperparameters\nparam_grid = {'k': [30, 40, 50], 'min_k': [3, 6, 9],\n              'sim_options': {'name': ['msd', 'cosine'],\n                              'user_based': [False]}\n              }\n\n# Performing 3-fold cross-validation to tune the hyperparameters\ngs = GridSearchCV(KNNBasic, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)\n\n# Fitting the data\ngs.fit(data)\n\n# Find the best RMSE score\nprint(gs.best_score['rmse'])\n\n# Extract the combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","metadata":{"executionInfo":{"elapsed":28706,"status":"ok","timestamp":1718343843383,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"f5bcZ3HgTsnt","outputId":"24050bc9-b401-495a-ac85-6108fa392304"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It:** How do the parameters affect the performance of the model? Can we improve the performance of the model further? Check the list of hyperparameters [here](https://surprise.readthedocs.io/en/stable/knn_inspired.html).","metadata":{"id":"SXLxjLEQYvWk"}},{"cell_type":"code","source":"# Apply the best model found in the grid search\nsim_options = {'name': 'msd',\n               'user_based': False}\n\n# Creating an instance of KNNBasic with optimal hyperparameter values\nsim_item_item_optimized = KNNBasic(sim_options = sim_options, k = 40, min_k = 6, random_state = 1, verbose = False)\n\n# Training the algorithm on the train set\nsim_item_item_optimized.fit(trainset)","metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718343843383,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"dSeiM1qeTsnt","outputId":"d432eadf-ed9e-4a36-a179-8440074f88bb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(sim_item_item_optimized)\n\n# Save the model's info\nsim_item_item_optimized_params = f\"k={sim_item_item_optimized.k}, min_k={sim_item_item_optimized.min_k}, sim_options={sim_item_item_optimized.sim_options}, verbose={sim_item_item_optimized.verbose}\"\nadd_model_record(df_model_info, 'Item-Item Optimized', sim_item_item_optimized_params, precision, recall, f1, rmse)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343846542,"user_tz":240,"elapsed":3177,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"534863ce-e1be-4936-f3bb-47033c4769ca","id":"s8JPiWZUuv0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- We can observe that after tuning hyperparameters, **F_1 score, Precision and Recall of the model are all slightly better than the baseline model**. Along with this, **the RMSE of the model has gone down a little bit in comparison to the model with default hyperparameters**. Hence, we can say that the model performance has improved after hyperparameter tuning.","metadata":{"id":"XxXelRIluvfh"}},{"cell_type":"code","source":"# Predict the play_count by a user(user_id 6958) for the song (song_id 1671)\nsim_item_item_optimized.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1718343846542,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"gIBRRvdoTsnt","outputId":"bdc9ddf6-badd-466b-85ee-1dfe42c9a580"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting play count for a sample user_id 6958 with song_id 3232 which is not listened to by the user\nsim_item_item_optimized.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1718343846542,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"LNEgcI9PTsnu","outputId":"5396b431-c120-4626-f943-004bc906ae97"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The predicted play count are exactly the same as the baseline model. Although the overall score has increased a little bit, the prediction for this particular user is still not that good.","metadata":{"id":"yf3kDSepuwcw"}},{"cell_type":"code","source":"# Find five most similar items to the item with inner id 0\nsim_item_item_optimized.get_neighbors(0, k = 5)","metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1718343846542,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"ZRJS4oDFTsnu","outputId":"9af0ee67-d8ee-4c54-fa4c-eeeaadfd26b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making top 5 recommendations for any user_id  with item_item_similarity-based recommendation engine\nrecommendations = get_recommendations(df_final, 6958, 5, sim_item_item_optimized)","metadata":{"id":"rzoEbuZFTsnu","executionInfo":{"status":"ok","timestamp":1718343846542,"user_tz":240,"elapsed":19,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the dataframe for above recommendations with columns \"song_id\" and \"predicted_play_count\"\npd.DataFrame(recommendations, columns = ['song_id', 'predicted_play_count'])","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1718343846543,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"_kXVTiysTsnv","outputId":"dbe1a1d1-3649-4cbb-c8fb-d486110d5df1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the ranking_songs function\nranking_songs(recommendations, final_play)","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718343846543,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"7gewfmTATsnv","outputId":"1207dfdd-32cf-4647-b420-16b555b80447"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n*   We use the make top n recommendations function for the same user with the new fitted model. It recommended different songs.\n*   The ranking_song function is used on this new set of recommended songs and with the corrected play count their ranking order are different.","metadata":{"id":"Ore9XTFgv5Np"}},{"cell_type":"markdown","source":"### Model Based Collaborative Filtering - Matrix Factorization","metadata":{"id":"rKgJpSA9vOOL"}},{"cell_type":"markdown","source":"Model-based Collaborative Filtering is a **personalized recommendation system**, the recommendations are based on the past behavior of the user and it is not dependent on any additional information. We use **latent features** to find recommendations for each user.","metadata":{"id":"hJynidJCw-ti"}},{"cell_type":"code","source":"# Build baseline model using svd\n\n# Using SVD with matrix factorization\nsvd = SVD(random_state = 1)\n\n# Training the algorithm on the training dataset\nsvd.fit(trainset)","metadata":{"executionInfo":{"elapsed":1376,"status":"ok","timestamp":1718343847904,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"07-2PT5Ssjqm","outputId":"9e645041-1e9e-4f37-dea5-092c57819242"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(svd)\n\n# Save the model's info\nsvd_params = f\"'n_epochs':{svd.n_epochs}, 'lr':{svd.lr_bu}, 'reg':{svd.reg_bu}\"\nadd_model_record(df_model_info, 'SVD', svd_params, precision, recall, f1, rmse)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343848270,"user_tz":240,"elapsed":368,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"b7e8441c-e0fa-4d79-8050-0a736fa00398","id":"NhA0Y2zYvlzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making prediction for user (with user_id 6958) to song (with song_id 1671), take r_ui = 2\nsvd.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718343848270,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"yWIhfdxXsjqm","outputId":"cf44b9da-3dcb-4b8e-fd4c-c96190e2ab7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a prediction for the user who has not listened to the song (song_id 3232)\nsvd.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718343848270,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"APm-uMSvcAMf","outputId":"d398b0b4-77b5-42aa-f71c-24fa062bff40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Improving matrix factorization based recommendation system by tuning its hyperparameters","metadata":{"id":"23tnRUJJxWTR"}},{"cell_type":"code","source":"# Set the parameter space to tune\nparam_grid = {'n_epochs': [10, 20, 30], 'lr_all': [0.001, 0.005, 0.01],\n              'reg_all': [0.2, 0.4, 0.6]}\n\n# Performe 3-fold grid-search cross-validation\ngs = GridSearchCV(SVD, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)\n\n# Fitting data\ngs.fit(data)\n\n# Best RMSE score\nprint(gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","metadata":{"executionInfo":{"elapsed":26047,"status":"ok","timestamp":1718343874313,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"4bM81V_hvtwv","outputId":"3ad7828a-4e65-497c-8008-1a08f8e45c99"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It**: How do the parameters affect the performance of the model? Can we improve the performance of the model further? Check the available hyperparameters [here](https://surprise.readthedocs.io/en/stable/matrix_factorization.html).\n\n\n*   The number of iterations (epochs) controls the number of times the model updates its parameters based on the training data. It can affect the model's convergence, risk of overfitting, computational cost, and overall performance.\n\n*   The learning rate determines the size of the steps taken during optimization. A too high or too low learning rate can lead to convergence issues or slow convergence, respectively. Optimizing the learning rate is crucial for achieving good performance.\n\n*   Regularization strength for all parameters helps prevent overfitting by penalizing large parameter values. A higher regularization strength encourages simpler models with smaller parameter values, reducing the risk of overfitting. The choice of an appropriate regularization strength depends on the specific dataset and the trade-off between bias and variance.\n\n","metadata":{"id":"aSgBRcL1xnVC"}},{"cell_type":"code","source":"# Building the optimized SVD model using optimal hyperparameters\nsvd_optimized = SVD(n_epochs = 30, lr_all = 0.01, reg_all = 0.2, random_state = 1)\n\n# Training the algorithm on the train set\nsvd_optimized = svd_optimized.fit(trainset)","metadata":{"id":"TA_7xe-nnhuu","executionInfo":{"status":"ok","timestamp":1718343876413,"user_tz":240,"elapsed":2105,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(svd_optimized)\n\n# Save the model's info\nsvd_optimized_params = gs.best_params\nadd_model_record(df_model_info, 'SVD Optimized', svd_optimized_params, precision, recall, f1, rmse)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343876847,"user_tz":240,"elapsed":440,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"051b9059-d75e-454b-bbca-53c4150b41aa","id":"jSsafq2RxYK8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- We can observe that after tuning hyperparameters, **F_1 score, Precision and Recall of the model are all slightly better than the baseline model**. Along with this, **the RMSE of the model has gone down a little bit in comparison to the model with default hyperparameters**. Hence, we can say that the model performance has improved after hyperparameter tuning.","metadata":{"id":"l3t5JdBmxz8l"}},{"cell_type":"code","source":"# Using svd_algo_optimized model to recommend for userId 6958 and song_id 1671\nsvd_optimized.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1718343876847,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"s6C1PAfboM8_","outputId":"168e801e-b810-4d16-93e4-1aec00f4e203"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using svd_algo_optimized model to recommend for userId 6958 and song_id 3232 with unknown baseline play_count\nsvd_optimized.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718343876847,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"k1xjn3kOoQyg","outputId":"315b2767-a37a-4780-f253-a44adea8ed56"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n\n*   The predicted play counts are close to the baseline model. Although the overall score has increased a little bit, the prediction for this particular user is still not that good.\n\n\n","metadata":{"id":"Qm732Wuvy76R"}},{"cell_type":"code","source":"# Getting top 5 recommendations for user_id 6958 using \"svd_optimized\" algorithm\nsvd_recommendations = get_recommendations(df_final, 6958, 5, svd_optimized)","metadata":{"id":"1LGeE2EB_n90","executionInfo":{"status":"ok","timestamp":1718343876847,"user_tz":240,"elapsed":10,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ranking songs based on above recommendations\nranking_songs(svd_recommendations, final_play)","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718343876847,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"6ngiGSJU818M","outputId":"9ab2786c-380d-4b04-8645-e96f00e94895"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:*\n\n*   We use the make top n recommendations function for the same user with the new fitted model. It recommended different songs.\n*   The ranking_song function is used on this new set of recommended songs and returned the corrected play count.","metadata":{"id":"SepUU1Efy_9Z"}},{"cell_type":"markdown","source":"### Cluster Based Recommendation System","metadata":{"id":"57b31de5"}},{"cell_type":"markdown","source":"In **clustering-based recommendation systems**, we explore the **similarities and differences** in people's tastes in songs based on how they rate different songs. We cluster similar users together and recommend songs to a user based on play_counts from other users in the same cluster.","metadata":{"id":"9Xv2AZCszCdN"}},{"cell_type":"code","source":"# Make baseline clustering model\n\n# Using CoClustering algorithm\nclust_baseline = CoClustering(random_state = 1)\n\n# Training the algorithm on the train set\nclust_baseline.fit(trainset)","metadata":{"executionInfo":{"elapsed":1823,"status":"ok","timestamp":1718343878663,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"0c4b20e4","outputId":"c25fecb3-55b7-48a6-c878-72ffda5a5e9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(clust_baseline)\n\n# Save the model's info\nclust_baseline_params = f\"n_cltr_u: {clust_baseline.n_cltr_u}, n_cltr_i: {clust_baseline.n_cltr_i}, n_epochs: {clust_baseline.n_epochs}\"\nadd_model_record(df_model_info, 'Cluster Based', clust_baseline_params, precision, recall, f1, rmse)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343879588,"user_tz":240,"elapsed":929,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"817c60d3-179c-4894-fa9e-9d5ad63e446f","id":"Cpjmozz_7v4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making prediction for user_id 6958 and song_id 1671\nclust_baseline.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718343879588,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"11dbdc0f","outputId":"09a07117-55a3-43a8-e833-abdf33ae291a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making prediction for user (userid 6958) for a song(song_id 3232) not listened to by the user\nclust_baseline.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718343879588,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"dab1aaed","outputId":"a26f6f5c-4d47-4690-8f2c-d819c7399b2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Improving clustering-based recommendation system by tuning its hyper-parameters","metadata":{"id":"c2fd66f5"}},{"cell_type":"code","source":"# Set the parameter space to tune\nparam_grid = {'n_cltr_u': [3, 4, 5, 6], 'n_cltr_i': [3, 4, 5, 6], 'n_epochs': [30, 40, 50]}\n\n# Performing 3-Fold gridsearch cross-validation\ngs = GridSearchCV(CoClustering, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)\n\n# Fitting data\ngs.fit(data)\n\n# Printing the best RMSE score\nprint(gs.best_score['rmse'])\n\n# Printing the combination of parameters that gives the best RMSE score\nprint(gs.best_params['rmse'])","metadata":{"executionInfo":{"elapsed":74994,"status":"ok","timestamp":1718343954577,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"efe7d8e6","outputId":"781532a0-c48c-4190-f42c-46fa3dcf8290"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Think About It**: How do the parameters affect the performance of the model? Can we improve the performance of the model further? Check the available hyperparameters [here](https://surprise.readthedocs.io/en/stable/co_clustering.html).","metadata":{"id":"CS6aMVJLyj21"}},{"cell_type":"code","source":"# Train the tuned Coclustering algorithm\n# Using tuned Coclustering algorithm\nclust_tuned = CoClustering(n_cltr_u = 3, n_cltr_i = 5, n_epochs = 50, random_state = 1)\n\n# Training the algorithm on the train set\nclust_tuned.fit(trainset)","metadata":{"executionInfo":{"elapsed":4501,"status":"ok","timestamp":1718343959060,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"5a7a8a30","outputId":"63461acb-a321-44c9-96e3-819ba8c381a4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us compute precision@k and recall@k also with k =30\nprecision, recall, f1, rmse = precision_recall_at_k(clust_tuned)\n\n# Save the model's info\nclust_tuned_params = gs.best_params\nadd_model_record(df_model_info, 'Cluster Tuned', clust_tuned_params, precision, recall, f1, rmse)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343959595,"user_tz":240,"elapsed":552,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"154e3955-684c-41a9-9cfb-edf39eed104c","id":"thDGkwSa9vqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model info dataframe\ndf_model_info","metadata":{"executionInfo":{"status":"ok","timestamp":1718343959595,"user_tz":240,"elapsed":35,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"36891bc9-0518-4d2a-fd90-3d6c9d4bed02","id":"n9pC-R0gK21s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n- We can observe that after tuning hyperparameters, **the RMSE, F_1 score, Precision and Recall of the model are all the same as the baseline model**.\n\n\n","metadata":{"id":"l-Jvce1gznKa"}},{"cell_type":"code","source":"# Using co_clustering_optimized model to recommend for userId 6958 and song_id 1671\nclust_tuned.predict(6958, 1671, r_ui = 2, verbose = True)","metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1718343959596,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"6ba5b26b","outputId":"8ef9f894-da4f-410b-9e88-f54e1ca4006c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use Co_clustering based optimized model to recommend for userId 6958 and song_id 3232 with unknown baseline play_count\nclust_tuned.predict(6958, 3232, verbose = True)","metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1718343959596,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"ec582940","outputId":"c7c2aed3-7e53-403d-89e6-10a44569c924"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n-   The prediction for this particular user is still not that good.","metadata":{"id":"rjGUSMqrzoDH"}},{"cell_type":"markdown","source":"#### Implementing the recommendation algorithm based on optimized CoClustering model","metadata":{"id":"df9e28ba"}},{"cell_type":"code","source":"# Getting top 5 recommendations for user_id 6958 using \"Co-clustering based optimized\" algorithm\nclustering_recommendations = get_recommendations(df_final, 6958, 5, clust_tuned)","metadata":{"id":"e0f36e15","executionInfo":{"status":"ok","timestamp":1718343959596,"user_tz":240,"elapsed":18,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correcting the play_count and Ranking the above songs","metadata":{"id":"f1696941"}},{"cell_type":"code","source":"# Ranking songs based on the above recommendations\nranking_songs(clustering_recommendations, final_play)","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1718343959596,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"c186f13b","outputId":"1e887da6-ba3d-451c-e196-587c6c29c03c","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n*   We use the make top n recommendations function for the same user with the new fitted model. It recommended different songs. But some songs are overlapping.\n*   The ranking_song function is used on this new set of recommended songs and returned the corrected play count. The prediction to the same song are similar to the previous model's.","metadata":{"id":"_uJ_nZjBzvKH"}},{"cell_type":"markdown","source":"### Content Based Recommendation Systems","metadata":{"id":"5U56oSNsR-F2"}},{"cell_type":"markdown","source":"**Think About It:** So far we have only used the play_count of songs to find recommendations but we have other information/features on songs as well. Can we take those song features into account?","metadata":{"id":"9aTEqaOjhoEg"}},{"cell_type":"code","source":"# Concatenate the \"title\", \"release\", \"artist_name\" columns to create a different column named \"text\"\ndf_final['text'] = df_final['title'] + ' ' + df_final['release'] + ' ' + df_final['artist_name']\n\ndf_final.head()","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718343959596,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"UX826CsjR-F3","outputId":"dacac21c-e451-4263-e8f0-9ae206989f4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the columns 'user_id', 'song_id', 'play_count', 'title', 'text' from df_small data\ndf_small = df_final[['user_id', 'song_id', 'play_count', 'title', 'text']]\n\n# Drop the duplicates from the title column\ndf_small = df_small.drop_duplicates(subset = ['title'])\n\n# Set the title column as the index\ndf_small = df_small.set_index('title')\n\n# See the first 5 records of the df_small dataset\ndf_small.head()","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718343959596,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"WdXw4U-wR-F4","outputId":"6da513b3-092e-4120-cd29-9584905fba5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the series of indices from the data\nindices = pd.Series(df_small.index)","metadata":{"id":"qDcYHwZTR-F5","executionInfo":{"status":"ok","timestamp":1718343959596,"user_tz":240,"elapsed":12,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary packages to work with text data\nimport nltk\nnltk.download('omw-1.4')\n\n# Download punkt library\nnltk.download('punkt')\n\n# Download stopwords library\nnltk.download('stopwords')\n\n# Download wordnet\nnltk.download('wordnet')\n\n# Import regular expression\nimport re\n\n# Import word_tokenizer\nfrom nltk import word_tokenize\n\n# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\n\n# Import stopwords\nfrom nltk.corpus import stopwords\n\n# Import CountVectorizer and TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1718343959973,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"9UINF3Nwvwfr","outputId":"ae40c538-85a1-4089-8585-58ee35cadb5d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a **function to pre-process the text data:**","metadata":{"id":"Jt2vitlnhoEg"}},{"cell_type":"code","source":"# Create a function to tokenize the text\ndef tokenize(text):\n\n    # Making each letter as lowercase and removing non-alphabetical text\n    text = re.sub(r\"[^a-zA-Z]\",\" \", text.lower())\n\n    # Extracting each word in the text\n    tokens = word_tokenize(text)\n\n    # Removing stopwords\n    words = [word for word in tokens if word not in stopwords.words(\"english\")]\n\n    # Lemmatize the words\n    text_lems = [WordNetLemmatizer().lemmatize(lem).strip() for lem in words]\n\n    return text_lems","metadata":{"id":"j5QSSeUvR-F6","executionInfo":{"status":"ok","timestamp":1718343959973,"user_tz":240,"elapsed":8,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create tfidf vectorizer\ntfidf = TfidfVectorizer(tokenizer = tokenize)\n\n# Fit_transfrom the above vectorizer on the text column and then convert the output into an array\nsong_tfidf = tfidf.fit_transform(df_small['text'].values).toarray()","metadata":{"id":"RI_onIGdR-F6","executionInfo":{"status":"ok","timestamp":1718343961024,"user_tz":240,"elapsed":1058,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the cosine similarity for the tfidf above output\nsimilar_songs = cosine_similarity(song_tfidf, song_tfidf)\n\n# Let us see the above array\nsimilar_songs","metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1718343961024,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"Beak6ODRR-F7","outputId":"6ce81f0c-0dfe-4bba-b0f5-0e3a9bc344f3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Finally, let's create a function to find most similar songs to recommend for a given song.","metadata":{"id":"3Jjo3UHKhoEh"}},{"cell_type":"code","source":"# Function that takes in song title as input and returns the top 10 recommended songs\ndef recommendations(title, similar_songs):\n\n    recommended_songs = []\n\n    indices = pd.Series(df_small.index)\n\n    # Getting the index of the song that matches the title\n    idx = indices[indices == title].index[0]\n\n    # Creating a Series with the similarity scores in descending order\n    score_series = pd.Series(similar_songs[idx]).sort_values(ascending = False)\n\n    # Getting the indexes of the 10 most similar songs\n    top_10_indexes = list(score_series.iloc[1 : 11].index)\n    print(top_10_indexes)\n\n    # Populating the list with the titles of the best 10 matching songs\n    for i in top_10_indexes:\n        recommended_songs.append(list(df_small.index)[i])\n\n    return recommended_songs\n","metadata":{"id":"upANOISkR-F8","executionInfo":{"status":"ok","timestamp":1718343961024,"user_tz":240,"elapsed":30,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recommending 10 songs similar to Learn to Fly","metadata":{"id":"o4EINBmkR-F8"}},{"cell_type":"code","source":"# Make the recommendation for the song with title 'Learn To Fly'\nrecommendations('Learn To Fly', similar_songs)","metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1718343961024,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"},"user_tz":240},"id":"ohEK5dkVR-F8","outputId":"a55c23aa-429f-44b0-a01e-ce034ac7f8d1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The song 'Learn To Fly' belongs to **Rock** genres, and the **majority of our recommendations** lie in this genres. It implies that the resulting recommendation system is working well.","metadata":{"id":"UQ7iI5QJ0oem"}},{"cell_type":"markdown","source":"### Hybrid Recommendation Systems","metadata":{"id":"BXxox6lYvbn6"}},{"cell_type":"markdown","source":"When dealing with sparse datasets in recommendation systems, combining the predictions from multiple models to create a hybrid model can improve performance. Below we will be implementing 3 different regression methods to build hybrid recommendation systems.  ","metadata":{"id":"QviVdmF3vbn6"}},{"cell_type":"code","source":"# Importing libraries for building linear regression model\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV as SklearnGridSearchCV","metadata":{"id":"Js27umrUvbn6","executionInfo":{"status":"ok","timestamp":1718343961025,"user_tz":240,"elapsed":29,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract predictions from models\ndef hybrid_extract(models, dataset):\n\n    # Dictionary to store results\n    pred_dict = {}\n\n    for model_index, model in enumerate(models):\n\n        # Get predictions for the current model\n        predictions = model.test(dataset)\n\n        # Loop through each prediction\n        for pred in predictions:\n            uid = pred.uid\n            iid = pred.iid\n            est = pred.est\n            act = pred.r_ui\n\n            # Create a unique key for each uid-iid pair\n            key = (uid, iid)\n\n            # Initialize the dictionary entry if it doesn't exist\n            if key not in pred_dict:\n                pred_dict[key] = {'user_id': uid, 'song_id': iid}\n                pred_dict[key]['actual'] = act\n\n            # Store the model's prediction in the dictionary\n            pred_dict[key][f'prediction_{model_index+1}'] = est\n\n\n    # Convert to DataFrame\n    df_pred = pd.DataFrame.from_dict(pred_dict, orient='index')\n\n    # Reset index to flatten the DataFrame\n    df_pred.reset_index(drop=True, inplace=True)\n\n\n    # Prepare for later regression\n\n    # Extract only the model prediction columns\n    prediction_columns = [col for col in df_pred.columns if col.startswith('prediction_')]\n    X = df_pred[prediction_columns]\n\n    y = df_pred['actual']\n\n    return X,y,df_pred","metadata":{"id":"8m9XR3plvbn6","executionInfo":{"status":"ok","timestamp":1718343961025,"user_tz":240,"elapsed":28,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mark down Regression results and Evaluate the performance of the regression models\ndef hybrid_process(X, dataset, regression_model):\n\n    # Predict play count using the hybrid model\n    dataset['hybrid_prediction'] = regression_model.predict(X)\n\n    # Calc mse of the regression models\n    mse = round(mean_squared_error(dataset['actual'], dataset['hybrid_prediction']),3)\n\n    # Print mse of the regression models\n    print('---------------------------------------------------------------------------------------------------------')\n    print(f'Regression Mean Squared Error: {mse}')\n\n    # Print coefficients and intercept\n    print(f'Regression Coefficients: {regression_model.coef_}')\n    print(f'Regression Intercept   : {regression_model.intercept_}')\n\n    return dataset\n","metadata":{"id":"IoWkY4zcvbn6","executionInfo":{"status":"ok","timestamp":1718343961025,"user_tz":240,"elapsed":28,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\ndef hybrid_precision_recall_at_k(df_pred, k=30, threshold=1.5):\n    \"\"\"Return precision and recall and F_1 score at k metrics for each user\"\"\"\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n\n    for index, row in df_pred.iterrows():\n        uid = row['user_id']\n        true_r = row['actual']\n        est = row['hybrid_prediction']\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, playing_count in user_est_true.items():\n\n        # Sort play count by estimated value\n        playing_count.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in playing_count)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in playing_count[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in playing_count[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        # When n_rec_k is 0, Precision is undefined. We here set Precision to 0 when n_rec_k is 0.\n\n        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n\n        # Recall@K: Proportion of relevant items that are recommended\n        # When n_rel is 0, Recall is undefined. We here set Recall to 0 when n_rel is 0.\n\n        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n\n    #Mean of all the predicted precisions are calculated.\n    precision = round((sum(prec for prec in precisions.values()) / len(precisions)),8)\n    #Mean of all the predicted recalls are calculated.\n    recall = round((sum(rec for rec in recalls.values()) / len(recalls)),8)\n    # Formula to compute the F-1 score.\n    f1 = round((2*precision*recall)/(precision+recall),8)\n\n\n    # Calculate RMSE for the predictions\n    y_true = df_pred['actual']\n    y_pred = df_pred['hybrid_prediction']\n\n    rmse = round(mean_squared_error(y_true, y_pred, squared=False),8)\n\n    print('---------------------------------------------------------------------------------------------------------')\n    print('RMSE: ', rmse)  # Print the RMSE\n    print('Precision: ', precision) #Command to print the overall precision\n    print('Recall: ', recall) #Command to print the overall recall\n    print('F_1 score: ', f1) # Formula to compute the F-1 score.\n    print('---------------------------------------------------------------------------------------------------------')\n\n    # Return all 4 scores\n    return precision, recall, f1, rmse","metadata":{"id":"2fpW7gkpvbn6","executionInfo":{"status":"ok","timestamp":1718343961025,"user_tz":240,"elapsed":28,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check predictions for a user with an song\ndef hybrid_predict(models, uid, iid, regression_model, verbose=True, pca_model=None):\n    # verbose to control to print the preditions or not\n    # Initialize the new record with uid and iid\n    new_record = {\n    'uid': uid,\n    'iid': iid,\n    'hybrid_prediction': 0\n    }\n\n    # Loop through the models and add the predictions to the new record\n    for model_index, model in enumerate(models):\n        prediction_key = f'prediction_{model_index + 1}'\n        new_record[prediction_key] = model.predict(uid, iid).est\n\n\n    # Convert to data frame\n    df_merged1 = pd.DataFrame([new_record])\n    # Prepare data for linear regression\n    prediction_columns = [col for col in df_merged1.columns if col.startswith('prediction_')]\n    X1 = df_merged1[prediction_columns]\n\n    if pca_model is not None:\n      # transform using the fitted PCA\n      X1_pca = pca.transform(X1)\n      # Predict ratings using the hybrid model\n      regression_model_pred = regression_model.predict(X1_pca)\n    else:\n      # Predict ratings using the hybrid model\n      regression_model_pred = regression_model.predict(X1)\n\n    df_merged1['hybrid_prediction'] = regression_model_pred\n\n    # Display the DataFrame with hybrid predictions\n    if (verbose==True):\n        print(df_merged1)\n\n    return regression_model_pred\n","metadata":{"id":"DdUJ7tqYvbn7","executionInfo":{"status":"ok","timestamp":1718343961025,"user_tz":240,"elapsed":28,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hybrid_get_recommendations(data, user_id, top_n, models, regression_model, pca_model=None):\n\n    # Creating an empty list to store the recommended song ids\n    recommendations = []\n\n    # Creating an user item interactions matrix\n    user_item_interactions_matrix = data.pivot(index = 'user_id', columns = 'song_id', values = 'play_count')\n\n    # Extracting those song ids which the user_id has not played yet\n    non_interacted_songs = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n\n    # Looping through each of the song ids which user_id has not interacted yet\n    for song_id in non_interacted_songs:\n\n        # Predicting the users for those non played song ids by this user\n        est = hybrid_predict(models, user_id, song_id, regression_model, verbose=False, pca_model=pca_model)\n\n        # Appending the predicted play_counts\n        recommendations.append((song_id, est))\n\n    # Sorting the predicted play_counts in descending order\n    recommendations.sort(key = lambda x: x[1], reverse = True)\n\n    return recommendations[:top_n] # Returing top n highest predicted play_count songs for this user","metadata":{"id":"V9hx6E1Kf0Ix","executionInfo":{"status":"ok","timestamp":1718343961025,"user_tz":240,"elapsed":27,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to combine the 4 optimized models.","metadata":{"id":"1h_V7xWsvbn7"}},{"cell_type":"code","source":"# Combine all models\nmodels = [svd_optimized, sim_user_user_optimized, sim_item_item_optimized, clust_tuned]\nmodel_combination = 'SVD, U-U, I-I, Co-Clustering'\n\n# Get predictions from models for the training set\ntrainset_tuples = [(trainset.to_raw_uid(iuid), trainset.to_raw_iid(iiid), trainset.ur[iuid][jj][1])\n                for (iuid, iiratings) in trainset.ur.items()\n                for jj, (iiid, _) in enumerate(iiratings)]\n\nX_train, y_train, df_trainset_pred = hybrid_extract(models, trainset_tuples)\n# Create a deep copy to backup the init. value\ndf_trainset_pred_init = df_trainset_pred.copy(deep=True)\n\n# Get predictions from models for the test set\nX_test, y_test, df_testset_pred = hybrid_extract(models, testset)\n# Create a deep copy to backup the init. value\ndf_testset_pred_init = df_testset_pred.copy(deep=True)","metadata":{"id":"dcxUXtzNDA1I","executionInfo":{"status":"ok","timestamp":1718343992553,"user_tz":240,"elapsed":31556,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the correlation of the predictions\nplt.figure(figsize=(10,5))\nsns.heatmap(X_train.corr(),annot=True,cmap='Spectral',vmin=-1,vmax=1)\nplt.show()","metadata":{"id":"Iraca9T41f2w","executionInfo":{"status":"ok","timestamp":1718343993252,"user_tz":240,"elapsed":719,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"8a5d5d08-62e3-48b0-87a9-5659880eaadb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use PCA to remove correlation before applying linear regression.","metadata":{"id":"a6ba99yrTSo6"}},{"cell_type":"code","source":"# Handle correlation with PCA\npca = PCA(n_components=len(models))\nX_train_pca = pca.fit_transform(X_train)\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Total explained variance:\", np.sum(pca.explained_variance_ratio_))\nX_test_pca = pca.transform(X_test)","metadata":{"id":"MJi7GXWBTEUa","executionInfo":{"status":"ok","timestamp":1718343993252,"user_tz":240,"elapsed":22,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"c2a5f296-b665-4879-9f10-7ae359f0e8a5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert PCA result to DataFrame\nX_train_pca_df = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n# Show the correlation after PCA\nplt.figure(figsize=(10,5))\nsns.heatmap(X_train_pca_df.corr(),annot=True,cmap='Spectral',vmin=-1,vmax=1)\nplt.show()","metadata":{"id":"AKKMYCN31mZU","executionInfo":{"status":"ok","timestamp":1718343993253,"user_tz":240,"elapsed":20,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"3e57f37b-7e97-4786-9c5a-5e23215236f9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After applying PCA, the resulting principal components are orthogonal to each other, effectively removing any correlations present in the original feature space.","metadata":{"id":"LWfIOIVl2fmU"}},{"cell_type":"markdown","source":"#### Hybrid - Linear Regression recommendation system","metadata":{"id":"UjrL_qTGUMuZ"}},{"cell_type":"code","source":"# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Train a linear regression model using the PCA components\nlr = LinearRegression()\nlr.fit(X_train_pca, y_train)\n\ndf_trainset_pred = hybrid_process(X_train_pca, df_trainset_pred, lr)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test_pca, df_testset_pred, lr)","metadata":{"id":"YG1IOBpDkQMY","executionInfo":{"status":"ok","timestamp":1718343993253,"user_tz":240,"elapsed":19,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"82e3afd8-e393-43cc-a21a-342bda2acde3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\nHybrid_params = lr.get_params()\nadd_model_record(df_model_info, 'Hybrid - PCA and Linear Regression', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343995929,"user_tz":240,"elapsed":2693,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"56ea4a9f-e9e3-4382-ecd4-3c0a5953ef93","id":"BUpXtsgpLTOV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Recall score has improved greatly! It increased to 0.819! Let's look into the detailed predictions for both train set and test set.","metadata":{"id":"T1i9RxuVwLe-"}},{"cell_type":"code","source":"df_trainset_pred","metadata":{"outputId":"94828e5e-02a3-4bd8-8742-46d84e67a3cb","executionInfo":{"status":"ok","timestamp":1718343996292,"user_tz":240,"elapsed":367,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"SChM5on2xMhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_testset_pred","metadata":{"outputId":"69c80910-7155-4246-b886-09991bdcd22c","executionInfo":{"status":"ok","timestamp":1718343996292,"user_tz":240,"elapsed":12,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"w-mY0N8kxMh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check predictions for a user with an interacted song\nh_pred = hybrid_predict(models, 6958, 1671, lr, pca_model=pca)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343996292,"user_tz":240,"elapsed":11,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"Y9o8dzSlv4QV","outputId":"931ae1a7-d387-4fb0-b646-8abb21782f4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks pretty good. The PCA and Linear Regression predicted 1.8~ is very close the actual 2.0 play count.","metadata":{"id":"scTf4dnJv4QV"}},{"cell_type":"code","source":"# Check predictions for a user with an not-interacted song\nh_pred = hybrid_predict(models, 6958, 3232, lr, pca_model=pca)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343996292,"user_tz":240,"elapsed":8,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"tSVLlQk7v4QV","outputId":"7d0e11ff-ecbd-4d2d-bc97-02138d1ce1dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making top 5 recommendations for a user_id with hybrid recommendation engine\nhybrid_recommendations = hybrid_get_recommendations(df_final, 6958, 5, models, lr, pca_model=pca)\n\n# Applying the ranking_songs function\nranking_songs(hybrid_recommendations, final_play)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343997537,"user_tz":240,"elapsed":1252,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"N0t3dWUYv4QV","outputId":"9090385a-7ddf-48d9-b9d7-41bccfad0736"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- We use PCA and Linear Regression to build a hybrid model based on the 4 optimized models.\n- The Hybrid model improve the performance greatly! The recall score increased to **0.819**! And F1 scores have been increased too. Though Precision score decreased a bit, and RMSE increased a bit.\n- We implement the prediction and recommendation function for the hybrid model.","metadata":{"id":"bTEPGMXDzSDN"}},{"cell_type":"markdown","source":"#### Hybrid - Ridge Regression recommendation system","metadata":{"id":"aBWCYhNaUgdB"}},{"cell_type":"code","source":"# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Train a Ridge regression model using the PCA components\nridge = Ridge()\nridge.fit(X_train, y_train)\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, ridge)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, ridge)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343997537,"user_tz":240,"elapsed":10,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"037f27c6-8fab-4da7-8242-631aa9e5a56f","id":"kods62nHxfgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\nHybrid_params = ridge.get_params()\nadd_model_record(df_model_info, 'Hybrid - Ridge Regression', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718343999501,"user_tz":240,"elapsed":1971,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"bf169c9b-6954-4abb-f981-f21d4131aa8b","id":"G2OgjZFWPdmJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not much difference with the Linear Regression model. Let's try to tune it.\n\n#### Improving Hybrid - Ridge Regression recommendation system by tuning its hyper-parameters","metadata":{"id":"v7MoFcO8Uoxs"}},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------------------------------------\n# Tune the Ridge Regression\n#------------------------------------------------------------------------------------------------------------------------------------------\n# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Define Ridge regression model\nridge_optimized = Ridge()\n\n# Define hyperparameters to tune\nparam_grid_ridge = {\n    'alpha': [0.1, 1.0, 10.0, 100.0],\n    'fit_intercept': [True, False],\n    'max_iter': [None, 1000, 2000],\n    'tol': [0.001, 0.0001],\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n}\n\n# Perform grid search with cross-validation\ngrid_search_ridge = SklearnGridSearchCV(ridge_optimized, param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\ngrid_search_ridge.fit(X_train, y_train)\n\n# Best estimator\nprint(\"Ridge regression - Best estimator :\")\nprint(grid_search_ridge.best_estimator_)\n\n# Best parameters\nprint(\"Ridge regression - Best parameters:\")\nprint(grid_search_ridge.best_params_)\n\n# Best score\nprint(\"Ridge regression - Best score     :\")\nprint(grid_search_ridge.best_score_)\n\nridge_optimized = grid_search_ridge.best_estimator_\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, ridge_optimized)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, ridge_optimized)","metadata":{"id":"wYCiGAo5pqNl","executionInfo":{"status":"ok","timestamp":1718344092711,"user_tz":240,"elapsed":93217,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"c43e16c6-c06f-4f82-eab5-1249b04bd95b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\n#Hybrid_params = grid_search_ridge.best_params_\nHybrid_params = str({**grid_search_ridge.best_params_,**ridge_optimized.get_params()})\nadd_model_record(df_model_info, 'Hybrid - Ridge Regression Optimized', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344095379,"user_tz":240,"elapsed":2699,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"22e9233e-9a9c-4e67-8085-6bf790be513a","id":"ybXT8qX_qh_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check predictions for a user with an interacted song\nh_pred = hybrid_predict(models, 6958, 1671, ridge_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344095379,"user_tz":240,"elapsed":8,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"9a47a74b-d32a-4013-e541-77bf1beff0b0","id":"L2Lttr5knG2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks pretty good. The Ridge model predicted 1.8~ is very close the actual 2.0 play count.","metadata":{"id":"Ah8CmEVknG2F"}},{"cell_type":"code","source":"# Check predictions for a user with an not-interacted song\nh_pred = hybrid_predict(models, 6958, 3232, ridge_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344095379,"user_tz":240,"elapsed":5,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"0aa9b0ee-f121-4847-a960-614a91c70efc","id":"YZysqAQPnG2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making top 5 recommendations for a user_id with hybrid recommendation engine\nhybrid_recommendations = hybrid_get_recommendations(df_final, 6958, 5, models, ridge_optimized)\n\n# Applying the ranking_songs function\nranking_songs(hybrid_recommendations, final_play)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344096579,"user_tz":240,"elapsed":1204,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"1a2345ef-7256-40b5-9cb4-d20fb849b629","id":"WaJVL3XDnG2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The Ridge regression models return almost the same scores, not much difference with the linear regression ones. The Optimized one got a little bit higher F1 score and Precision score, and the lowest RMSE.\n","metadata":{"id":"ZDf0Brv5zt3S"}},{"cell_type":"markdown","source":"#### Hybrid - Lasso Regression recommendation system","metadata":{"id":"MfGMbMGIU5Ie"}},{"cell_type":"code","source":"# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Train a Lasso regression model using the PCA components\nlasso = Lasso()\nlasso.fit(X_train, y_train)\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, lasso)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, lasso)\n\n","metadata":{"id":"aN6Cr-Rpzs01","executionInfo":{"status":"ok","timestamp":1718344096579,"user_tz":240,"elapsed":19,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"2838fb0c-fa85-41ad-9c15-537c02bc4385"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)","metadata":{"id":"C9a63vbqd1ZT","executionInfo":{"status":"ok","timestamp":1718344098115,"user_tz":240,"elapsed":1547,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"6c5688fa-0c47-4dc7-d5c9-dcd2eed8e233"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is something werid in the Lasso regression. Let's check any Constant Predictor exist.","metadata":{"id":"WTNXdYLuvbn7"}},{"cell_type":"code","source":"# check if the column has the same value for all rows\nconstant_columns = [col for col in df_trainset_pred.columns if df_trainset_pred[col].nunique() == 1]\nprint(\"Constant columns:\", constant_columns)","metadata":{"id":"gUF_sRqkvbn7","outputId":"1f8ec450-55c5-4fb7-8376-cba63ee92476","executionInfo":{"status":"ok","timestamp":1718344098468,"user_tz":240,"elapsed":362,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trainset_pred","metadata":{"outputId":"a1c5024d-c288-49be-88a9-2c5ce15a8332","executionInfo":{"status":"ok","timestamp":1718344098469,"user_tz":240,"elapsed":14,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"TcE58nWh2256"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_testset_pred","metadata":{"outputId":"2bd5aab1-6b06-4476-f38c-4cdebb7c9e91","executionInfo":{"status":"ok","timestamp":1718344098469,"user_tz":240,"elapsed":12,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"YfkRd5ln226R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the actual play count's mean\nprint(df_trainset_pred['actual'].mean())\nprint(df_testset_pred['actual'].mean())","metadata":{"id":"P8sSg34rvbn7","outputId":"6aa6cb98-978a-4398-bc70-f6ed304d36ef","executionInfo":{"status":"ok","timestamp":1718344098469,"user_tz":240,"elapsed":11,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, the Lasso regression model is **overshrinking**, driving all feature coefficients to zero, it only predicts the mean of the target variable, becoming a constant predictor.\n\nLet's try to tune it.","metadata":{"id":"tBKiFsWWvbn7"}},{"cell_type":"markdown","source":"#### Improving Hybrid - Lasso Regression recommendation system by tuning its hyper-parameters","metadata":{"id":"aNpq96ObUwF4"}},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------------------------------------\n# Tune the Lasso Regression\n#------------------------------------------------------------------------------------------------------------------------------------------\n# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Define lasso regression model\nlasso_optimized = Lasso()\n\n# Define hyperparameters to tune\nparam_grid_lasso = {\n  'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],    # Regularization strength for Lasso\n  'max_iter': [1000, 2000, 3000],            # Maximum number of iterations\n  'tol': [0.0001, 0.001, 0.01]               # Tolerance for stopping criteria\n}\n\n# Perform grid search with cross-validation\ngrid_search_lasso = SklearnGridSearchCV(lasso_optimized, param_grid_lasso, cv=5, scoring='neg_mean_squared_error')\ngrid_search_lasso.fit(X_train, y_train)\n\n# Best estimator\nprint(\"Lasso regression - Best estimator :\")\nprint(grid_search_lasso.best_estimator_)\n\n# Best parameters\nprint(\"Lasso regression - Best parameters:\")\nprint(grid_search_lasso.best_params_)\n\n# Best score\nprint(\"Lasso regression - Best score     :\")\nprint(grid_search_lasso.best_score_)\n\nlasso_optimized = grid_search_lasso.best_estimator_\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, lasso_optimized)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, lasso_optimized)","metadata":{"id":"cMhmOjBDtL5i","executionInfo":{"status":"ok","timestamp":1718344104165,"user_tz":240,"elapsed":5704,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"1cfabd5a-979b-4490-aa39-7a895b19a1aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\n#Hybrid_params = grid_search_lasso.best_params_\nHybrid_params = str({**grid_search_lasso.best_params_,**lasso_optimized.get_params()})\nadd_model_record(df_model_info, 'Hybrid - Lasso Regression Optimized', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344106114,"user_tz":240,"elapsed":1966,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"7c1b9e45-b746-41fc-b00c-7db7415697a7","id":"ZxDlfahptSaN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if the column has the same value for all rows\nconstant_columns = [col for col in df_trainset_pred.columns if df_trainset_pred[col].nunique() == 1]\nprint(\"Constant columns:\", constant_columns)","metadata":{"outputId":"b37d5107-9140-4c71-882f-d2e6e4ea59ae","executionInfo":{"status":"ok","timestamp":1718344106114,"user_tz":240,"elapsed":17,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"e48VZwOZ0MAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tuning solved the shrinking problem. Let's try the prediction and recommendation function too.","metadata":{"id":"KwOYU3_c0-_q"}},{"cell_type":"code","source":"# Check predictions for a user with an interacted song\nh_pred = hybrid_predict(models, 6958, 1671, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344106466,"user_tz":240,"elapsed":362,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"a3560e54-464e-4e2b-ac84-019d999f5d61","id":"K4KMqDHc0qUA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks pretty good. The Lasso model predicted 1.7~ is very close the actual 2.0 play count.","metadata":{"id":"_pBgUhZ40qUM"}},{"cell_type":"code","source":"# Check predictions for a user with an not-interacted song\nh_pred = hybrid_predict(models, 6958, 3232, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344106466,"user_tz":240,"elapsed":18,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"22edb291-bdab-4c8e-f0a4-6b7f1bc53558","id":"RDmummCF0qUM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making top 5 recommendations for a user_id with hybrid recommendation engine\nhybrid_recommendations = hybrid_get_recommendations(df_final, 6958, 5, models, lasso_optimized)\n\n# Applying the ranking_songs function\nranking_songs(hybrid_recommendations, final_play)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344107839,"user_tz":240,"elapsed":1385,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"b0d67910-2d4f-4ec2-b255-e987be3015aa","id":"-QpU3cDZ0qUM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- Implement Linear, Ridge and Lasso regression to combine the 4 optimized models to build hybrid recommendation systems and also tune them.\n\n- Evaluation method is included so that we can compare all models, especially with the 4 basic models.\n\n- All the hybrid models have much higher recall scores than the stand-alone models.\n\n- Met the overshrinking problem in Lasso models. But after tuned, the problem has been solved.\n\n- So far we have 4 usable hybrid models, their performance are quite similar, all have **high recall score of ~0.81 and F1 score of ~0.53**, which are **higher** than all the stand-alone models. Though the precision score is lower than the stand-alone models except for Item-Item optimized.\n\n- **The Tuned Lasso Regression model** got a a bit lower Recall score than the others, but the precision scores are a bit higher, that makes it got a bit higher F1 score. And RMSE is the lowest. It's the **best-performing model** so far.\n\n\n\n","metadata":{"id":"N5Y-6KTSQ5ux"}},{"cell_type":"markdown","source":"#### Hybrid - combine SVD and User-User only\n\nAs SVD and User-User model has higher precision score, let's  combine these two to to see if there is any improvement.","metadata":{"id":"4tccnW_l3kvz"}},{"cell_type":"code","source":"# Combine 2 models only\nmodels = [svd_optimized, sim_user_user_optimized]\nmodel_combination = 'SVD, U-U'\n\nX_train, y_train, df_trainset_pred = hybrid_extract(models, trainset_tuples)\n# Create a deep copy to backup the init. value\ndf_trainset_pred_init = df_trainset_pred.copy(deep=True)\n\n# Get predictions from models for the test set\nX_test, y_test, df_testset_pred = hybrid_extract(models, testset)\n# Create a deep copy to backup the init. value\ndf_testset_pred_init = df_testset_pred.copy(deep=True)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344133204,"user_tz":240,"elapsed":25375,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"EU4KZDet5UTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle correlation with PCA\npca = PCA(n_components=len(models))\nX_train_pca = pca.fit_transform(X_train)\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Total explained variance:\", np.sum(pca.explained_variance_ratio_))\nX_test_pca = pca.transform(X_test)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344133204,"user_tz":240,"elapsed":75,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"8e0bb288-dcc2-4797-fd36-e4abbf56e061","id":"JVGx55u65uxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Train a linear regression model using the PCA components\nlr = LinearRegression()\nlr.fit(X_train_pca, y_train)\n\ndf_trainset_pred = hybrid_process(X_train_pca, df_trainset_pred, lr)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test_pca, df_testset_pred, lr)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344133204,"user_tz":240,"elapsed":22,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"2c984670-9d24-4f68-bc3c-de90e6f20ec2","id":"vvt68slv6L62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\nHybrid_params = lr.get_params()\nadd_model_record(df_model_info, 'Hybrid - PCA and Linear Regression', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344136016,"user_tz":240,"elapsed":2832,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"39ff0961-e238-48b8-f007-9734a70c6a91","id":"NFUYGAcp6L7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------------------------------------\n# Tune the Ridge Regression\n#------------------------------------------------------------------------------------------------------------------------------------------\n# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Define Ridge regression model\nridge_optimized = Ridge()\n\n# Define hyperparameters to tune\nparam_grid_ridge = {\n    'alpha': [0.1, 1.0, 10.0, 100.0],\n    'fit_intercept': [True, False],\n    'max_iter': [None, 1000, 2000],\n    'tol': [0.001, 0.0001],\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n}\n\n# Perform grid search with cross-validation\ngrid_search_ridge = SklearnGridSearchCV(ridge_optimized, param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\ngrid_search_ridge.fit(X_train, y_train)\n\n# Best estimator\nprint(\"Ridge regression - Best estimator :\")\nprint(grid_search_ridge.best_estimator_)\n\n# Best parameters\nprint(\"Ridge regression - Best parameters:\")\nprint(grid_search_ridge.best_params_)\n\n# Best score\nprint(\"Ridge regression - Best score     :\")\nprint(grid_search_ridge.best_score_)\n\nridge_optimized = grid_search_ridge.best_estimator_\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, ridge_optimized)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, ridge_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344205842,"user_tz":240,"elapsed":69833,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"402b6d50-a0b1-4c49-c947-0c00a05aeec6","id":"4IxfhELj8bPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\n#Hybrid_params = grid_search_ridge.best_params_\nHybrid_params = str({**grid_search_ridge.best_params_,**ridge_optimized.get_params()})\nadd_model_record(df_model_info, 'Hybrid - Ridge Regression Optimized', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344209213,"user_tz":240,"elapsed":3383,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"118dbca7-fe3c-4a2a-ad64-da500b567a8d","id":"-GwPY-um8bPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------------------------------------\n# Tune the Lasso Regression\n#------------------------------------------------------------------------------------------------------------------------------------------\n# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Define lasso regression model\nlasso_optimized = Lasso()\n\n# Define hyperparameters to tune\nparam_grid_lasso = {\n  'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],    # Regularization strength for Lasso\n  'max_iter': [1000, 2000, 3000],            # Maximum number of iterations\n  'tol': [0.0001, 0.001, 0.01]               # Tolerance for stopping criteria\n}\n\n# Perform grid search with cross-validation\ngrid_search_lasso = SklearnGridSearchCV(lasso_optimized, param_grid_lasso, cv=5, scoring='neg_mean_squared_error')\ngrid_search_lasso.fit(X_train, y_train)\n\n# Best estimator\nprint(\"Lasso regression - Best estimator :\")\nprint(grid_search_lasso.best_estimator_)\n\n# Best parameters\nprint(\"Lasso regression - Best parameters:\")\nprint(grid_search_lasso.best_params_)\n\n# Best score\nprint(\"Lasso regression - Best score     :\")\nprint(grid_search_lasso.best_score_)\n\nlasso_optimized = grid_search_lasso.best_estimator_\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, lasso_optimized)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344213730,"user_tz":240,"elapsed":4520,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"d19e08b7-4293-4613-b6a9-c8b5ab04e4d0","id":"oqgxyjMw8uhe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\n#Hybrid_params = grid_search_lasso.best_params_\nHybrid_params = str({**grid_search_lasso.best_params_,**lasso_optimized.get_params()})\nadd_model_record(df_model_info, 'Hybrid - Lasso Regression Optimized', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344215168,"user_tz":240,"elapsed":1452,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"585fb211-224a-4eee-f96c-f8af5831af78","id":"AchrNl728uhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if the column has the same value for all rows\nconstant_columns = [col for col in df_trainset_pred.columns if df_trainset_pred[col].nunique() == 1]\nprint(\"Constant columns:\", constant_columns)","metadata":{"outputId":"663ab8d3-e1ce-4896-9ffb-adb39fa466c5","executionInfo":{"status":"ok","timestamp":1718344215168,"user_tz":240,"elapsed":6,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"Ln5DHIzN8uhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check predictions for a user with an interacted song\nh_pred = hybrid_predict(models, 6958, 1671, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344215168,"user_tz":240,"elapsed":4,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"d39d96fd-3d0c-47d5-ac01-496e5570fb19","id":"V3Vv_0gHAoPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks pretty good. The Lasso model predicted 1.7~ is very close the actual 2.0 play count.","metadata":{"id":"gW8CZRviAoP3"}},{"cell_type":"code","source":"# Check predictions for a user with an not-interacted song\nh_pred = hybrid_predict(models, 6958, 3232, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344215168,"user_tz":240,"elapsed":3,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"de05f4fa-0253-4d35-d55e-aaf33c4483be","id":"1pcJ_0FXAoP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making top 5 recommendations for a user_id with hybrid recommendation engine\nhybrid_recommendations = hybrid_get_recommendations(df_final, 6958, 5, models, lasso_optimized)\n\n# Applying the ranking_songs function\nranking_songs(hybrid_recommendations, final_play)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344216596,"user_tz":240,"elapsed":1430,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"542e0848-fbf0-4361-df33-9e355b3cab78","id":"VQx1R9XJAoP3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The precision score has **increased to ~0.397**. The F1 score has increased to **~0.53**.","metadata":{"id":"6OLBzr0E_A6Z"}},{"cell_type":"markdown","source":"#### Hybrid - combine User-User and Item-item only\n\nAs User-User and Item-item model complement each other, let's  combine these two to see if there is any improvement.","metadata":{"id":"ryVq_x1E9F0j"}},{"cell_type":"code","source":"# Combine 2 models only\nmodels = [sim_user_user_optimized, sim_item_item_optimized]\nmodel_combination = 'U-U, I-I'\n\nX_train, y_train, df_trainset_pred = hybrid_extract(models, trainset_tuples)\n# Create a deep copy to backup the init. value\ndf_trainset_pred_init = df_trainset_pred.copy(deep=True)\n\n# Get predictions from models for the test set\nX_test, y_test, df_testset_pred = hybrid_extract(models, testset)\n# Create a deep copy to backup the init. value\ndf_testset_pred_init = df_testset_pred.copy(deep=True)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344246503,"user_tz":240,"elapsed":29910,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"IpMGWFQw9F0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle correlation with PCA\npca = PCA(n_components=len(models))\nX_train_pca = pca.fit_transform(X_train)\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Total explained variance:\", np.sum(pca.explained_variance_ratio_))\nX_test_pca = pca.transform(X_test)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344246503,"user_tz":240,"elapsed":19,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"3a14e7ac-713e-4fad-e7d0-c0230b166069","id":"1gUMovWS9F0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Train a linear regression model using the PCA components\nlr = LinearRegression()\nlr.fit(X_train_pca, y_train)\n\ndf_trainset_pred = hybrid_process(X_train_pca, df_trainset_pred, lr)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test_pca, df_testset_pred, lr)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344246503,"user_tz":240,"elapsed":14,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"afa2a27a-e307-4815-c91c-ac960239a0b6","id":"GdSCiBpm9F0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\nHybrid_params = lr.get_params()\nadd_model_record(df_model_info, 'Hybrid - PCA and Linear Regression', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344248534,"user_tz":240,"elapsed":2044,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"d4f99a91-5b31-437b-8759-f765adbe38ef","id":"KwHD1CeG9F0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------------------------------------\n# Tune the Ridge Regression\n#------------------------------------------------------------------------------------------------------------------------------------------\n# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Define Ridge regression model\nridge_optimized = Ridge()\n\n# Define hyperparameters to tune\nparam_grid_ridge = {\n    'alpha': [0.1, 1.0, 10.0, 100.0],\n    'fit_intercept': [True, False],\n    'max_iter': [None, 1000, 2000],\n    'tol': [0.001, 0.0001],\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n}\n\n# Perform grid search with cross-validation\ngrid_search_ridge = SklearnGridSearchCV(ridge_optimized, param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\ngrid_search_ridge.fit(X_train, y_train)\n\n# Best estimator\nprint(\"Ridge regression - Best estimator :\")\nprint(grid_search_ridge.best_estimator_)\n\n# Best parameters\nprint(\"Ridge regression - Best parameters:\")\nprint(grid_search_ridge.best_params_)\n\n# Best score\nprint(\"Ridge regression - Best score     :\")\nprint(grid_search_ridge.best_score_)\n\nridge_optimized = grid_search_ridge.best_estimator_\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, ridge_optimized)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, ridge_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344318584,"user_tz":240,"elapsed":70053,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"5d9871f9-afb7-4cf5-e5cc-1fddcfe0401e","id":"LNoWuQrb9F0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\n#Hybrid_params = grid_search_ridge.best_params_\nHybrid_params = str({**grid_search_ridge.best_params_,**ridge_optimized.get_params()})\nadd_model_record(df_model_info, 'Hybrid - Ridge Regression Optimized', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344321244,"user_tz":240,"elapsed":2677,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"06b37176-c236-487f-a595-f12ad106b709","id":"pD0Mr-XT9F0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------------------------------------\n# Tune the Lasso Regression\n#------------------------------------------------------------------------------------------------------------------------------------------\n# Get predictions from models for the training set (recover from backup)\ndf_trainset_pred = df_trainset_pred_init\n\n# Get predictions from models for the test set (recover from backup)\ndf_testset_pred = df_testset_pred_init\n\n# Define lasso regression model\nlasso_optimized = Lasso()\n\n# Define hyperparameters to tune\nparam_grid_lasso = {\n  'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],    # Regularization strength for Lasso\n  'max_iter': [1000, 2000, 3000],            # Maximum number of iterations\n  'tol': [0.0001, 0.001, 0.01]               # Tolerance for stopping criteria\n}\n\n# Perform grid search with cross-validation\ngrid_search_lasso = SklearnGridSearchCV(lasso_optimized, param_grid_lasso, cv=5, scoring='neg_mean_squared_error')\ngrid_search_lasso.fit(X_train, y_train)\n\n# Best estimator\nprint(\"Lasso regression - Best estimator :\")\nprint(grid_search_lasso.best_estimator_)\n\n# Best parameters\nprint(\"Lasso regression - Best parameters:\")\nprint(grid_search_lasso.best_params_)\n\n# Best score\nprint(\"Lasso regression - Best score     :\")\nprint(grid_search_lasso.best_score_)\n\nlasso_optimized = grid_search_lasso.best_estimator_\n\ndf_trainset_pred = hybrid_process(X_train, df_trainset_pred, lasso_optimized)\n\n# Predict play count using the hybrid model\nprint('---------------------------------------------------------------------------------------------------------')\nprint('Apply on Test Set:')\ndf_testset_pred = hybrid_process(X_test, df_testset_pred, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344326283,"user_tz":240,"elapsed":5041,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"06ebe25f-c3a9-4cc5-dc06-77de628d126e","id":"_HsHwI3o9F0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calc precision@k, recall@k, and F_1 score\nprecision, recall, f1, rmse = hybrid_precision_recall_at_k(df_testset_pred, k=30, threshold=1.5)\n\n# Save the model's info\n#Hybrid_params = grid_search_lasso.best_params_\nHybrid_params = str({**grid_search_lasso.best_params_,**lasso_optimized.get_params()})\nadd_model_record(df_model_info, 'Hybrid - Lasso Regression Optimized', Hybrid_params, precision, recall, f1, rmse, model_combination)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344329138,"user_tz":240,"elapsed":2860,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"2066afd1-4452-4d1c-df6b-f6dfcfaa9e38","id":"K4a7vV5r9F0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if the column has the same value for all rows\nconstant_columns = [col for col in df_trainset_pred.columns if df_trainset_pred[col].nunique() == 1]\nprint(\"Constant columns:\", constant_columns)","metadata":{"outputId":"b7ebc879-a919-4c5e-ccf9-90b2a0eb3a94","executionInfo":{"status":"ok","timestamp":1718344329138,"user_tz":240,"elapsed":6,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"id":"EQutkDkj9F0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check predictions for a user with an interacted song\nh_pred = hybrid_predict(models, 6958, 1671, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344329138,"user_tz":240,"elapsed":4,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"fe6760c8-a024-4842-fc85-78becb0fae0b","id":"C3zlw4FDAXMZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks pretty good. The Lasso model predicted 1.7~ is very close the actual 2.0 play count.","metadata":{"id":"2gVNA5U2AXMx"}},{"cell_type":"code","source":"# Check predictions for a user with an not-interacted song\nh_pred = hybrid_predict(models, 6958, 3232, lasso_optimized)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344329138,"user_tz":240,"elapsed":3,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"42bd707f-16d0-4ce4-c4eb-b45788f6f228","id":"NGkhEiCZAXMy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making top 5 recommendations for a user_id with hybrid recommendation engine\nhybrid_recommendations = hybrid_get_recommendations(df_final, 6958, 5, models, lasso_optimized)\n\n# Applying the ranking_songs function\nranking_songs(hybrid_recommendations, final_play)","metadata":{"executionInfo":{"status":"ok","timestamp":1718344329862,"user_tz":240,"elapsed":726,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"af777c6a-c714-4ffc-854c-7881782fa287","id":"TB4hzga9AXMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n- The precision score has **increased to ~0.400**. F1 score is **~0.53**.","metadata":{"id":"24QV17EQ_erg"}},{"cell_type":"markdown","source":"#### Hybrid - Random Forest recommendation system","metadata":{"id":"JMRlZQb7W20H"}},{"cell_type":"markdown","source":"Let's build a recommendation system based on four basic optimized models. The main idea is to prioritize songs that are recommended more frequently, giving them a higher rank.","metadata":{"id":"f-EQBxC9WOwE"}},{"cell_type":"code","source":"def get_recommendations_forest(data, final_play, user_id, top_n, models):\n\n    # Initialize an empty list to hold all DataFrames\n    all_ranked_songs = []\n\n    for model_index, model in enumerate(models):\n        recommendations = get_recommendations(data, user_id, top_n, model)\n        ranked_songs = ranking_songs(recommendations, final_play)\n\n        #df_ranked_songs = pd.DataFrame(ranked_songs)\n        # Add the model and user_id information\n        ranked_songs['model'] = f'model_{model_index + 1}'\n        ranked_songs['uid'] = user_id\n\n        # Append the DataFrame to the list\n        all_ranked_songs.append(ranked_songs)\n\n    # Concatenate all DataFrames in the list into a single DataFrame\n    df_ranked_songs = pd.concat(all_ranked_songs, ignore_index=True)\n\n    # reset the index\n    df_ranked_songs.reset_index(drop=True, inplace=True)\n\n    # Group by 'song_id' and calculate the mean of 'play_count' and value counts\n    df_ranked_songs_count = df_ranked_songs.groupby('song_id').agg(\n        mean_play_count=('corrected_play_count', 'mean'),\n        value_count=('song_id', 'size')\n    ).reset_index()\n\n    # Sort by 'value_count' in descending order, mean_play_count in descending order\n    df_ranked_songs_count_sorted = df_ranked_songs_count.sort_values(by=['value_count', 'mean_play_count'], ascending=[False, False]).reset_index()\n\n    return df_ranked_songs_count_sorted[:top_n] # Returing top n songs for this user","metadata":{"id":"pGiXsz4K7uPJ","executionInfo":{"status":"ok","timestamp":1718344329862,"user_tz":240,"elapsed":5,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [svd_optimized, sim_user_user_optimized, sim_item_item_optimized, clust_tuned]\nforest_recommendations = get_recommendations_forest(df_final, final_play, 6958, 5, models)\nprint(forest_recommendations)","metadata":{"id":"si_7-iM37uPJ","executionInfo":{"status":"ok","timestamp":1718344331386,"user_tz":240,"elapsed":1529,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"08310403-2902-4600-d96e-f7c64ef3f268"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations and Insights:**\n\n\n\n*   We implement a function to Make top n recommendations for any user id based on four basic optimized models. The final ranking will first be sorted by the number of recommendations in descending order and then by the average play count prediction in descending order. It will return the song ids and aver. predicted play counts.\n*   The function inherently uses the corrected play count, applying fewer penalties to frequently played songs and more penalties to less frequently played songs.\n\n","metadata":{"id":"80LOuogVez-Q"}},{"cell_type":"markdown","source":"#### Comparing all the models","metadata":{"id":"XnIkmyVCXPQX"}},{"cell_type":"code","source":"# Check the model info dataframe\ndf_model_info","metadata":{"executionInfo":{"status":"ok","timestamp":1718344331387,"user_tz":240,"elapsed":21,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"34d0eb0e-8014-41a3-9534-751eb3aa375a","id":"Okf-_KjvVN-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine 'Model Type' and 'Combination' into a new 'Display Model Type' column\ndf_model_info['Display Model Type'] = df_model_info['Model Type'] + \" - \" + df_model_info['Combination']\n\n# Set the 'Display Model Type' column as the index for plotting\ndf_model_info.set_index('Display Model Type', inplace=True)\n\n# Plot the line plot for each metric in the same chart\nplt.figure(figsize=(14, 12))\n\nplt.plot(df_model_info.index, df_model_info['Precision'], marker='o', label='Precision')\nplt.plot(df_model_info.index, df_model_info['Recall'], marker='s', label='Recall')\nplt.plot(df_model_info.index, df_model_info['F1'], marker='^', label='F1')\nplt.plot(df_model_info.index, df_model_info['RMSE'], marker='d', label='RMSE')\n\nplt.title('Performance Metrics by Model Type')\nplt.xlabel('Model Type')\nplt.ylabel('Scores')\nplt.xticks(rotation=90)\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"iy9NYQuGDtHV","executionInfo":{"status":"ok","timestamp":1718344331387,"user_tz":240,"elapsed":16,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"70b7e9fb-6b39-43a8-bbb8-41de096b0283"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Conclusion and Recommendations**","metadata":{"id":"73367782"}},{"cell_type":"markdown","source":"\nWe build various recommendation systems:\n- Popularity-Based\n- Similarity-Based Collaborative filtering (user-user and item-item)\n- Matrix Factorization Based Collaborative Filtering\n- Clustering-based\n- Content-based collaborative filtering\n- Hybrid (combine SVD, user-user, item-item and Clustering-based, with Linear, Ridge and Lasso regression)\n- Hybrid - Random Forest","metadata":{"id":"N5BT7Ocwqf5x"}},{"cell_type":"markdown","source":"**Insights:**\n\n- Our dataset contains limited information about the songs and users themselves, necessitating a heavy reliance on user-song interactions to uncover latent features and clustering patterns.\n\n- We face the challenge of data sparsity which is common in recommendation systems.\n\n- Among these techniques, hybrid models generally outperform others in sparse data scenarios by integrating various data sources and methodologies, and leveraging the complementary strengths of different approaches.\n\n- Matrix Factorization models also show strong performance due to their ability to uncover latent factors and generalize well with limited data.\n\n- Content-based and simple collaborative filtering methods may lag behind in sparse datasets due to their reliance on either detailed metadata or sufficient user-item interactions.","metadata":{"id":"wjc6vTcoqp6v"}},{"cell_type":"markdown","source":"**Proposal for the final solution design:**\n\n*   **Hybrid model which combines SVD and user-user with Ridge regression**\n\n*   It has the **highest F1 score (0.530484)** and **high recall score (0.798430)** among all the models we tried, precision score is close the other models', making it the **best-performing model**.\n\n*   High recall can lead to more engagement, as users find more songs that they enjoy. This is particularly important in competitive markets where retaining user attention is crucial.\n\n*   *Hybrid model which combines SVD, user-user, item-item and Clustering-based model with Lasso regression* maybe the *2nd choice*. As It has the **highest recall score (0.819106)** and **high F1 score (0.525687)** among all the models we tried.\n   \n\n","metadata":{"id":"hK6PMGUtoxVx"}},{"cell_type":"markdown","source":"**Scope to improve:**\n\n- Enhancing Data Quality and Quantity: Collecting more interaction data and enriching item and user metadata can significantly improve model performance.\n\n- Parameter Tuning and Optimization: Fine-tuning hyperparameters and optimizing model architecture can yield better results.\n\n- Advanced Techniques: Incorporating advanced machine learning techniques, such as deep learning (e.g., neural collaborative filtering, autoencoders) and graph-based methods (e.g., Graph Convolutional Networks), can enhance the model's ability to capture complex patterns.\n\n- User Feedback Integration: Continuously incorporating user feedback and engagement data can refine recommendations and adapt to changing user preferences.\n\n- Regular Updates and Retraining: Regularly updating and retraining models with new data helps maintain accuracy and relevance over time.","metadata":{"id":"6ir04GrNW-GY"}}]}